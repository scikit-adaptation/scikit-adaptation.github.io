{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# How to use SKADA\n\nThis is a short example to get started with SKADA and perform domain adaptation\non a simple dataset. It illustrates the API choice specific to DA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Remi Flamary\n#\n# License: BSD 3-Clause\n# sphinx_gallery_thumbnail_number = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\nfrom skada import (\n    CORAL,\n    CORALAdapter,\n    GaussianReweightAdapter,\n    PerDomain,\n    SelectSource,\n    SelectSourceTarget,\n    make_da_pipeline,\n    source_target_split,\n)\nfrom skada.datasets import make_shifted_datasets\nfrom skada.metrics import PredictionEntropyScorer\nfrom skada.model_selection import SourceTargetShuffleSplit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DA dataset\n\nWe generate a simple 2D DA dataset. Note that DA datasets provided by SKADA\nare organized as follows:\n\n* :code:`X` is the input data, including the source and the target samples\n* :code:`y` is the output data to be predicted (labels on target samples are not\n  used when fitting the DA estimator)\n* :code:`sample_domain` encodes the domain of each sample (integer >=0 for\n  source and <0 for target)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get DA dataset\nX, y, sample_domain = make_shifted_datasets(\n    20, 20, shift=\"conditional_shift\", random_state=42\n)\n\n# split source and target for visualization\nXs, Xt, ys, yt = source_target_split(X, y, sample_domain=sample_domain)\nsample_domain_s = np.ones(Xs.shape[0])\nsample_domain_t = -np.ones(Xt.shape[0]) * 2\n\n# plot data\nplt.figure(1, (10, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(Xs[:, 0], Xs[:, 1], c=ys, cmap=\"tab10\", vmax=9, label=\"Source\")\nplt.title(\"Source data\")\nax = plt.axis()\n\nplt.subplot(1, 2, 2)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yt, cmap=\"tab10\", vmax=9, label=\"Target\")\nplt.axis(ax)\nplt.title(\"Target data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DA Classifier estimator\n\nSKADA estimators are used like scikit-learn estimators. The only difference is\nthat the :code:`sample_domain` array must be passed by name when fitting the\nestimator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create a DA estimator\nclf = CORAL()\n\n# train on all data\nclf.fit(X, y, sample_domain=sample_domain)\n\n# estimator is designed to predict on target by default\nyt_pred = clf.predict(Xt)\n\n# accuracy on source and target\nprint(\"Accuracy on source:\", clf.score(Xs, ys))\nprint(\"Accuracy on target:\", clf.score(Xt, yt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DA estimator in a pipeline\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# SKADA estimators can be used as the final estimator of a scikit-learn pipeline.\n# Again, the only difference is that the :code:`sample_domain` array must be passed\n# by name during in fit.\n\n\n# create a DA pipeline\npipe = make_pipeline(StandardScaler(), CORAL(base_estimator=SVC()))\npipe.fit(X, y, sample_domain=sample_domain)\n\nprint(\"Accuracy on target:\", pipe.score(Xt, yt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DA Adapter pipeline\n\nSeveral SKADA estimators include a data adapter that transforms the input data\nso that a scikit-learn estimator can be used. For those methods, SKADA\nprovides a :code:`Adapter` class that can be used in a DA pipeline from\n:code:`make_da_pipeline`.\n\nHere is an example with the CORAL and GaussianReweight adapters.\n\n.. WARNING::\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#   Note that as illustrated below for reweighting adapters, one needs a\n#   subsequent estimator that takes :code:`sample_weight` as an input parameter.\n#   This can be done using the :code:`set_fit_request` method of the estimator\n#   by calling :code:`.set_fit_request(sample_weight=True)`.\n#   If the estimator (for pipeline or DA estimator) does not\n#   require sample weights, the DA pipeline will raise an error.\n\n\n# create a DA pipeline with CORAL adapter\npipe = make_da_pipeline(StandardScaler(), CORALAdapter(), SVC())\npipe.fit(X, y, sample_domain=sample_domain)\n\nprint(\"Accuracy on target:\", pipe.score(Xt, yt))\n\n# create a DA pipeline with GaussianReweight adapter (does not work well on\n# conditional shift).\npipe = make_da_pipeline(\n    StandardScaler(),\n    GaussianReweightAdapter(),\n    LogisticRegression().set_fit_request(sample_weight=True),\n)\npipe.fit(X, y, sample_domain=sample_domain)\n\nprint(\"Accuracy on target:\", pipe.score(Xt, yt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DA estimators with score cross-validation\n\nDA estimators are compatible with scikit-learn cross-validation functions.\nNote that the :code:`sample_domain` array must be passed in the :code:`params`\ndictionary of the :code:`cross_val_score` function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# splitter for cross-validation of score\ncv = SourceTargetShuffleSplit(random_state=0)\n\n# DA scorer not using target labels (not available in DA)\nscorer = PredictionEntropyScorer()\n\nclf = CORAL(SVC(probability=True))  # needs probability for entropy score\n\n# cross-validation\nscores = cross_val_score(\n    clf, X, y, params={\"sample_domain\": sample_domain}, cv=cv, scoring=scorer\n)\n\nprint(f\"Entropy score: {scores.mean():1.2f} (+-{scores.std():1.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DA estimator with grid search\n\nDA estimators are also compatible with scikit-learn grid search functions.\nNote that the :code:`sample_domain` array must be passed in the :code:`fit`\nmethod of the grid search.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reg_coral = [0.1, 0.5, 1, \"auto\"]\n\nclf = make_da_pipeline(StandardScaler(), CORALAdapter(), SVC(probability=True))\n\n# grid search\ngrid_search = GridSearchCV(\n    estimator=clf,\n    param_grid={\"coraladapter__reg\": reg_coral},\n    cv=SourceTargetShuffleSplit(random_state=0),\n    scoring=PredictionEntropyScorer(),\n)\n\ngrid_search.fit(X, y, sample_domain=sample_domain)\n\nprint(\"Best regularization parameter:\", grid_search.best_params_[\"coraladapter__reg\"])\nprint(\"Accuracy on target:\", np.mean(grid_search.predict(Xt) == yt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced DA pipeline\n\nThe DA pipeline can be used with any estimator and any adapter. But more\nimportantly all estimators in the pipeline are automatically wrapped in what\nwe call in skada a `Selector`. The selector is a wrapper that allows you to\nchoose which data is passed during fit and predict/transform.\n\nIn the following example, one StandardScaler is trained per domain. Then\na single SVC is trained on source data only. When predicting on target data the\npipeline will automatically use the StandardScaler trained on target and the SVC\ntrained on source.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create a DA pipeline with SelectSourceTarget estimators\n\npipe = make_da_pipeline(\n    SelectSourceTarget(StandardScaler()),\n    SelectSource(SVC()),\n)\n\npipe.fit(X, y, sample_domain=sample_domain)\n\nprint(\"Accuracy on source:\", pipe.score(Xs, ys, sample_domain=sample_domain_s))\nprint(\"Accuracy on target:\", pipe.score(Xt, yt))  # target by default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly one can use the PerDomain selector to train a different estimator\nper domain. This allows to handle multiple source and target domains. In this\ncase :code:`sample_domain` must be provided to fit and predict/transform.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipe = make_da_pipeline(\n    PerDomain(StandardScaler()),\n    SelectSource(SVC()),\n)\n\npipe.fit(X, y, sample_domain=sample_domain)\n\nprint(\"Accuracy on all data:\", pipe.score(X, y, sample_domain=sample_domain))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One can use a default selector on the whole pipeline which allows for\ninstance to train the whole pipeline only on the source data as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipe_train_on_source = make_da_pipeline(\n    StandardScaler(),\n    SVC(),\n    default_selector=SelectSource,\n)\n\npipe_train_on_source.fit(X, y, sample_domain=sample_domain)\nprint(\"Accuracy on source:\", pipe_train_on_source.score(Xs, ys))\nprint(\"Accuracy on target:\", pipe_train_on_source.score(Xt, yt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One can also use a default selector on the whole pipeline but overwrite it for\nthe last estimator. In the example below a :code:`StandardScaler` and a\n:code:`PCA` are estimated per domain but the final SVC is trained on source data only.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipe_perdomain = make_da_pipeline(\n    StandardScaler(),\n    PCA(n_components=2),\n    SelectSource(SVC()),\n    default_selector=SelectSourceTarget,\n    mask_target_labels=False,\n)\n\npipe_perdomain.fit(X, y, sample_domain=sample_domain)\nprint(\n    \"Accuracy on source:\", pipe_perdomain.score(Xs, ys, sample_domain=sample_domain_s)\n)\nprint(\n    \"Accuracy on target:\", pipe_perdomain.score(Xt, yt, sample_domain=sample_domain_t)\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}