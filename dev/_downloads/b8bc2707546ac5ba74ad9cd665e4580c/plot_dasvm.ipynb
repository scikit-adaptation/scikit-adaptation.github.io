{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# DASVM classifier example\n\nThis example illustrates the DASVM method from [21].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Ruben Bueno\n#\n# License: BSD 3-Clause\n# sphinx_gallery_thumbnail_number = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.lines as mlines\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.base import clone\nfrom sklearn.svm import SVC\n\nfrom skada import source_target_split\nfrom skada._self_labeling import DASVMClassifier\nfrom skada.datasets import make_dataset_from_moons_distribution\n\nRANDOM_SEED = 42\n\n# base_estimator can be any classifier equipped with `decision_function` such as:\n# SVC(kernel='poly'), SVC(kernel='linear'), LogisticRegression(random_state=0), etc...\n# however the estimator has been created only for SVC.\nbase_estimator = SVC()\n\ntarget_marker = \"s\"\nsource_marker = \"o\"\nxlim = (-1.5, 2.4)\nylim = (-1, 1.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## We generate our 2D dataset with 2 classes\n\nWe generate a simple 2D dataset from a moon distribution, where source and target\nare not taken from the same location in the moons. This dataset thus presents a\ncovariate shift.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y, sample_domain = make_dataset_from_moons_distribution(\n    pos_source=[0.1, 0.2, 0.3, 0.4],\n    pos_target=[0.6, 0.7, 0.8, 0.9],\n    n_samples_source=10,\n    n_samples_target=10,\n    noise=0.1,\n    random_state=RANDOM_SEED,\n)\n\nXs, Xt, ys, yt = source_target_split(X, y, sample_domain=sample_domain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plots of the dataset\n\nAs we can see, the source and target datasets have different\ndistributions for the points' positions but have the same\nlabels for the same x-values.\nWe are then in the case of covariate shift.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "figure, axis = plt.subplots(1, 2, figsize=(10, 4))\n\naxis[0].scatter(Xs[:, 0], Xs[:, 1], c=ys, marker=source_marker)\naxis[0].set_xlim(xlim)\naxis[0].set_ylim(ylim)\naxis[0].set_title(\"source data points\")\n\naxis[1].scatter(Xt[:, 0], Xt[:, 1], c=yt, marker=target_marker)\naxis[1].set_xlim(xlim)\naxis[1].set_ylim(ylim)\naxis[1].set_title(\"target data points\")\n\nfigure.suptitle(\"data points\", fontsize=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage of the DASVMClassifier\n\nThe main problem here is that we only know the distribution of the points\nfrom the target dataset, our goal is to label it.\n\nThe DASVM method consist in fitting multiple base_estimator (SVC) by:\n    - Removing from the training dataset (if possible)\n      `k` points from the source dataset for which the current\n      estimator is doing well\n    - Adding to the training dataset (if possible) `k`\n      points from the target dataset for which out current\n      estimator is not so sure about it's prediction (those\n      are target points in the margin band, that are close to\n      the margin)\n    - Semi-labeling points that were added to the training set\n      and came from the target dataset\n    - Fit a new estimator on this training set\nHere we plot the progression of the SVC classifier when training with the DASVM\nalgorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "estimator = DASVMClassifier(\n    base_estimator=clone(base_estimator), k=5, save_estimators=True, save_indices=True\n).fit(X, y, sample_domain=sample_domain)\n\nepsilon = 0.01\nK = len(estimator.estimators) // 3\nfigure, axis = plt.subplots(2, 2, figsize=(2 * 5, 2 * 4))\naxis = np.concatenate((axis[0], axis[1]))\nfor i in list(range(0, len(estimator.estimators), K)) + [-1]:\n    j = i // K if i != -1 else -1\n    e = estimator.estimators[i]\n    x_points = np.linspace(xlim[0], xlim[1], 500)\n    y_points = np.linspace(ylim[0], ylim[1], 500)\n    X = np.array([[x, y] for x in x_points for y in y_points])\n\n    # plot margins\n    if j == -1:\n        X_ = X[np.absolute(e.decision_function(X) - 1) < epsilon]\n        axis[j].scatter(\n            X_[:, 0],\n            X_[:, 1],\n            c=[1] * X_.shape[0],\n            alpha=1,\n            cmap=\"gray\",\n            s=[0.1] * X_.shape[0],\n            label=\"margin\",\n        )\n        X_ = X[np.absolute(e.decision_function(X) + 1) < epsilon]\n        axis[j].scatter(\n            X_[:, 0],\n            X_[:, 1],\n            c=[1] * X_.shape[0],\n            alpha=1,\n            cmap=\"gray\",\n            s=[0.1] * X_.shape[0],\n        )\n        X_ = X[np.absolute(e.decision_function(X)) < epsilon]\n        axis[j].scatter(\n            X_[:, 0],\n            X_[:, 1],\n            c=[1] * X_.shape[0],\n            alpha=1,\n            cmap=\"autumn\",\n            s=[0.1] * X_.shape[0],\n            label=\"decision boundary\",\n        )\n    else:\n        X_ = X[np.absolute(e.decision_function(X) - 1) < epsilon]\n        axis[j].scatter(\n            X_[:, 0],\n            X_[:, 1],\n            c=[1] * X_.shape[0],\n            alpha=1,\n            cmap=\"gray\",\n            s=[0.1] * X_.shape[0],\n        )\n        X_ = X[np.absolute(e.decision_function(X) + 1) < epsilon]\n        axis[j].scatter(\n            X_[:, 0],\n            X_[:, 1],\n            c=[1] * X_.shape[0],\n            alpha=1,\n            cmap=\"gray\",\n            s=[0.1] * X_.shape[0],\n        )\n        X_ = X[np.absolute(e.decision_function(X)) < epsilon]\n        axis[j].scatter(\n            X_[:, 0],\n            X_[:, 1],\n            c=[1] * X_.shape[0],\n            alpha=1,\n            cmap=\"autumn\",\n            s=[0.1] * X_.shape[0],\n        )\n\n    X_s = Xs[~estimator.indices_source_deleted[i]]\n    X_t = Xt[estimator.indices_target_added[i]]\n    X = np.concatenate((X_s, X_t))\n\n    if sum(estimator.indices_target_added[i]) > 0:\n        semi_labels = e.predict(Xt[estimator.indices_target_added[i]])\n        axis[j].scatter(\n            X_s[:, 0],\n            X_s[:, 1],\n            c=ys[~estimator.indices_source_deleted[i]],\n            marker=source_marker,\n            alpha=0.7,\n        )\n        axis[j].scatter(\n            X_t[:, 0], X_t[:, 1], c=semi_labels, marker=target_marker, alpha=0.7\n        )\n    else:\n        semi_labels = np.array([])\n        axis[j].scatter(\n            X[:, 0], X[:, 1], c=ys[~estimator.indices_source_deleted[i]], alpha=0.7\n        )\n    X = Xt[~estimator.indices_target_added[i]]\n    axis[j].scatter(\n        X[:, 0],\n        X[:, 1],\n        cmap=\"gray\",\n        c=[0.5] * X.shape[0],\n        alpha=0.5,\n        vmax=1,\n        vmin=0,\n        marker=target_marker,\n    )\n\n    axis[j].set_xlim(xlim)\n    axis[j].set_ylim(ylim)\n    if i == -1:\n        i = len(estimator.estimators)\n    axis[j].set_title(f\"predictions at step {i}\")\n\nfigure.suptitle(\"evolutions of the predictions\", fontsize=20)\n\nmargin_line = mlines.Line2D(\n    [], [], color=\"black\", marker=\"_\", markersize=15, label=\"margin\"\n)\ndecision_boundary = mlines.Line2D(\n    [], [], color=\"red\", marker=\"_\", markersize=15, label=\"decision boundary\"\n)\naxis[0].legend(handles=[margin_line, decision_boundary], loc=\"lower left\")\naxis[-1].legend(handles=[margin_line, decision_boundary])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Labeling the target dataset\n\nHere we show 4 states from our algorithm, At first we are only given source\ndata points with label (which are circle, in colors showing the label), and\ntarget datapoints that have no labels (which are represented as squares, in\ngray when they have no labels)\n\nAs we go further in the algorithm steps, we can notice that more and more of\nthe target datapoints (squares) are now labeled, while more and more of the\nsource datapoints (circles) are removed from the training set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We show the improvement of the labeling technique.\nfigure, axis = plt.subplots(1, 2, figsize=(10, 4))\nsemi_labels = (base_estimator.fit(Xs, ys).predict(Xt), estimator.predict(Xt))\naxis[0].scatter(Xt[:, 0], Xt[:, 1], c=semi_labels[0], alpha=0.7, marker=target_marker)\naxis[1].scatter(Xt[:, 0], Xt[:, 1], c=semi_labels[1], alpha=0.7, marker=target_marker)\n\nscores = (\n    np.array([sum(semi_labels[0] == yt), sum(semi_labels[1] == yt)])\n    * 100\n    / semi_labels[0].shape[0]\n)\n\naxis[0].set_title(f\"Score without da methods: {scores[0]}%\")\naxis[1].set_title(f\"Score with DASVM: {scores[1]}%\")\nfigure.suptitle(\"predictions\")\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}