{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# JDOT Regressor and Classifier examples\n\nThis example shows how to use the JDOTRegressor [10] to learn a regression model\nfrom source to target domain on a simple concept drift 2D example. We use a\nsimple Kernel Ridge Regression (KRR) as base estimator.\n\nWe compare the performance of the KRR on the source and target domain, and the\nJDOTRegressor on the same task and illustrate the learned decision boundary and\nthe OT plan between samples estimated by JDOT.\n\n.. [10] Courty, N., Flamary, R., Habrard, A., & Rakotomamonjy, A. (2017). Joint\n        distribution optimal transportation for domain adaptation. Advances in\n        neural information processing systems, 30.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Remi Flamary\n#\n# License: BSD 3-Clause\n# sphinx_gallery_thumbnail_number = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import SVC\n\nfrom skada import JDOTClassifier, JDOTRegressor, source_target_split\nfrom skada.datasets import make_shifted_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate concept drift regression dataset and plot it\n\nWe generate a simple 2D concept drift dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y, sample_domain = make_shifted_datasets(\n    n_samples_source=20,\n    n_samples_target=20,\n    shift=\"concept_drift\",\n    noise=0.3,\n    label=\"regression\",\n    random_state=42,\n)\n\ny = (y - y.mean()) / y.std()\n\nXs, Xt, ys, yt = source_target_split(X, y, sample_domain=sample_domain)\n\n\nplt.figure(1, (10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(Xs[:, 0], Xs[:, 1], c=ys, label=\"Source\")\nplt.title(\"Source data\")\nax = plt.axis()\n\nplt.subplot(1, 2, 2)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yt, label=\"Target\")\nplt.title(\"Target data\")\nplt.axis(ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train a regressor on source data\n\nWe train a simple Kernel Ridge Regression (KRR) on the source domain and\nevaluate its performance on the source and target domain. Performance is\nmuch lower on the target domain due to the shift. We also plot the decision\nboundary learned by the KRR.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = KernelRidge(kernel=\"rbf\", alpha=0.5)\nclf.fit(Xs, ys)\n\n# Compute accuracy on source and target\nys_pred = clf.predict(Xs)\nyt_pred = clf.predict(Xt)\n\nmse_s = mean_squared_error(ys, ys_pred)\nmse_t = mean_squared_error(yt, yt_pred)\n\nprint(f\"MSE on source: {mse_s:.2f}\")\nprint(f\"MSE on target: {mse_t:.2f}\")\n\nXX, YY = np.meshgrid(np.linspace(ax[0], ax[1], 100), np.linspace(ax[2], ax[3], 100))\nZ = clf.predict(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)\n\n\nplt.figure(2, (10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(Xs[:, 0], Xs[:, 1], c=ys, label=\"Prediction\")\nplt.imshow(Z, extent=(ax[0], ax[1], ax[2], ax[3]), origin=\"lower\", alpha=0.5)\nplt.title(f\"KRR Prediction on source (MSE={mse_s:.2f})\")\nplt.axis(ax)\n\nplt.subplot(1, 2, 2)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yt, label=\"Prediction\")\nplt.imshow(Z, extent=(ax[0], ax[1], ax[2], ax[3]), origin=\"lower\", alpha=0.5)\nplt.title(f\"KRR Prediction on target (MSE={mse_t:.2f})\")\nplt.axis(ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train with JDOT regressor\n\nWe now use the JDOTRegressor to learn a regression model from source to\ntarget domain. We use the same KRR as base estimator. We compare the\nperformance of JDOT on the source and target domain, and illustrate the\nlearned decision boundary of JDOT. Performance is much better on the target\ndomain than with the KRR trained on source.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "jdot = JDOTRegressor(base_estimator=KernelRidge(kernel=\"rbf\", alpha=0.5), alpha=0.01)\n\njdot.fit(X, y, sample_domain=sample_domain)\n\nys_pred = jdot.predict(Xs)\nyt_pred = jdot.predict(Xt)\n\nmse_s = mean_squared_error(ys, ys_pred)\nmse_t = mean_squared_error(yt, yt_pred)\n\nZjdot = jdot.predict(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)\n\nprint(f\"JDOT MSE on source: {mse_s:.2f}\")\nprint(f\"JDOT MSE on target: {mse_t:.2f}\")\n\nplt.figure(3, (10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(Xs[:, 0], Xs[:, 1], c=ys, label=\"Prediction\")\nplt.imshow(Zjdot, extent=(ax[0], ax[1], ax[2], ax[3]), origin=\"lower\", alpha=0.5)\nplt.title(f\"JDOT Prediction on source (MSE={mse_s:.2f})\")\nplt.axis(ax)\n\nplt.subplot(1, 2, 2)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yt, label=\"Prediction\")\nplt.imshow(Zjdot, extent=(ax[0], ax[1], ax[2], ax[3]), origin=\"lower\", alpha=0.5)\nplt.title(f\"JDOT Prediction on target (MSE={mse_t:.2f})\")\nplt.axis(ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Illustration of the OT plan\n\nWe illustrate the OT plan between samples estimated by JDOT. We plot the\nOT plan between the source and target samples. We can see that the OT plan\nis able to align the source and target samples while preserving the label.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "T = jdot.sol_.plan\nT = T / T.max()\n\nplt.figure(4, (5, 5))\n\nplt.scatter(Xs[:, 0], Xs[:, 1], c=\"C0\", label=\"Source\", alpha=0.7)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=\"C1\", label=\"Target\", alpha=0.7)\n\nfor i in range(Xs.shape[0]):\n    for j in range(Xt.shape[0]):\n        if T[i, j] > 0.01:\n            plt.plot(\n                [Xs[i, 0], Xt[j, 0]], [Xs[i, 1], Xt[j, 1]], \"k\", alpha=T[i, j] * 0.8\n            )\nplt.legend()\nplt.title(\"OT plan between source and target\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate concept drift classification dataset and plot it\n\nWe generate a simple 2D concept drift dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y, sample_domain = make_shifted_datasets(\n    n_samples_source=20,\n    n_samples_target=20,\n    shift=\"concept_drift\",\n    noise=0.2,\n    label=\"multiclass\",\n    random_state=42,\n)\n\n\nXs, Xt, ys, yt = source_target_split(X, y, sample_domain=sample_domain)\n\n\nplt.figure(5, (10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(Xs[:, 0], Xs[:, 1], c=ys, cmap=\"tab10\", vmax=9, label=\"Source\")\nplt.title(\"Source data\")\nax = plt.axis()\n\nplt.subplot(1, 2, 2)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yt, cmap=\"tab10\", vmax=9, label=\"Target\")\nplt.title(\"Target data\")\nplt.axis(ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train a classifier on source data\n\nWe train a simple SVC classifier on the source domain and evaluate its\nperformance on the source and target domain. Performance is much lower on\nthe target domain due to the shift. We also plot the decision boundary\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression()\nclf.fit(Xs, ys)\n\n# Compute accuracy on source and target\nys_pred = clf.predict(Xs)\nyt_pred = clf.predict(Xt)\n\nacc_s = (ys_pred == ys).mean()\nacc_t = (yt_pred == yt).mean()\n\nprint(f\"Accuracy on source: {acc_s:.2f}\")\nprint(f\"Accuracy on target: {acc_t:.2f}\")\n\nXX, YY = np.meshgrid(np.linspace(ax[0], ax[1], 100), np.linspace(ax[2], ax[3], 100))\nZ = clf.predict(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)\n\n\nplt.figure(6, (10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(Xs[:, 0], Xs[:, 1], c=ys, cmap=\"tab10\", vmax=9, label=\"Prediction\")\nplt.imshow(\n    Z,\n    extent=(ax[0], ax[1], ax[2], ax[3]),\n    origin=\"lower\",\n    alpha=0.5,\n    cmap=\"tab10\",\n    vmax=9,\n)\nplt.title(f\"SVC Prediction on source (ACC={acc_s:.2f})\")\n\nplt.subplot(1, 2, 2)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yt, cmap=\"tab10\", vmax=9, label=\"Prediction\")\nplt.imshow(\n    Z,\n    extent=(ax[0], ax[1], ax[2], ax[3]),\n    origin=\"lower\",\n    alpha=0.5,\n    cmap=\"tab10\",\n    vmax=9,\n)\nplt.title(f\"SVC Prediction on target (ACC={acc_t:.2f})\")\nplt.axis(ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train with JDOT classifier\n\nWe now use the JDOTClassifier to learn a classification model from source to\ntarget domain. We use the same SVC as base estimator. We compare the\nperformance of JDOT on the source and target domain, and illustrate the\nlearned decision boundary of JDOT. Performance is much better on the target\ndomain than with the SVC trained on source.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "jdot = JDOTClassifier(LogisticRegression(), verbose=True)\n\njdot.fit(X, y, sample_domain=sample_domain)\n\nys_pred = jdot.predict(Xs)\nyt_pred = jdot.predict(Xt)\n\nacc_s = (ys_pred == ys).mean()\nacc_t = (yt_pred == yt).mean()\n\nprint(f\"JDOT Accuracy on source: {acc_s:.2f}\")\nprint(f\"JDOT Accuracy on target: {acc_t:.2f}\")\n\nXX, YY = np.meshgrid(np.linspace(ax[0], ax[1], 100), np.linspace(ax[2], ax[3], 100))\nZ = jdot.predict(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)\n\n\nplt.figure(7, (10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(Xs[:, 0], Xs[:, 1], c=ys, cmap=\"tab10\", vmax=9, label=\"Prediction\")\nplt.imshow(\n    Z,\n    extent=(ax[0], ax[1], ax[2], ax[3]),\n    origin=\"lower\",\n    alpha=0.5,\n    cmap=\"tab10\",\n    vmax=9,\n)\nplt.title(f\"JDOT reglog on source (ACC={acc_s:.2f})\")\n\nplt.subplot(1, 2, 2)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yt, cmap=\"tab10\", vmax=9, label=\"Prediction\")\nplt.imshow(\n    Z,\n    extent=(ax[0], ax[1], ax[2], ax[3]),\n    origin=\"lower\",\n    alpha=0.5,\n    cmap=\"tab10\",\n    vmax=9,\n)\nplt.title(f\"JDOT reglog on target (ACC={acc_t:.2f})\")\nplt.axis(ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train with JDOT classifier with SVC\n\nWe now use the JDOTClassifier with a support vector classifier as base\nestimator to learn a classification model from source to target domain.\nNote that in this case it is necessary to change the metric from the default\n'multinomial' to 'hinge' to match the hinge loss used by the SVC.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "jdot = JDOTClassifier(SVC(kernel=\"rbf\", C=1), metric=\"hinge\")\n\njdot.fit(X, y, sample_domain=sample_domain)\n\nys_pred = jdot.predict(Xs)\nyt_pred = jdot.predict(Xt)\n\nacc_s = (ys_pred == ys).mean()\nacc_t = (yt_pred == yt).mean()\n\nprint(f\"JDOT Accuracy on source: {acc_s:.2f}\")\nprint(f\"JDOT Accuracy on target: {acc_t:.2f}\")\n\nXX, YY = np.meshgrid(np.linspace(ax[0], ax[1], 100), np.linspace(ax[2], ax[3], 100))\nZ = jdot.predict(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)\n\n\nplt.figure(8, (10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(Xs[:, 0], Xs[:, 1], c=ys, cmap=\"tab10\", vmax=9, label=\"Prediction\")\nplt.imshow(\n    Z,\n    extent=(ax[0], ax[1], ax[2], ax[3]),\n    origin=\"lower\",\n    alpha=0.5,\n    cmap=\"tab10\",\n    vmax=9,\n)\nplt.title(f\"JDOT SVC on source (ACC={acc_s:.2f})\")\n\nplt.subplot(1, 2, 2)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yt, cmap=\"tab10\", vmax=9, label=\"Prediction\")\nplt.imshow(\n    Z,\n    extent=(ax[0], ax[1], ax[2], ax[3]),\n    origin=\"lower\",\n    alpha=0.5,\n    cmap=\"tab10\",\n    vmax=9,\n)\nplt.title(f\"JDOT SVC on target (ACC={acc_t:.2f})\")\nplt.axis(ax)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}