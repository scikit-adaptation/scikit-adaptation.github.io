{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Reweighting method example on covariate shift dataset\n\nAn example of the reweighting methods on a dataset subject\nto covariate shift\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author:   Ruben Bueno <ruben.bueno@polytechnique.edu>\n#           Antoine de Mathelin\n#\n# License: BSD 3-Clause\n# sphinx_gallery_thumbnail_number = 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KernelDensity\n\nfrom skada import (\n    DensityReweight,\n    DiscriminatorReweight,\n    GaussianReweight,\n    KLIEPReweight,\n    KMMReweight,\n    NearestNeighborReweight,\n    source_target_split,\n)\nfrom skada.datasets import make_shifted_datasets\nfrom skada.utils import extract_source_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The reweighting methods\nThe goal of reweighting methods is to estimate some weights for the\nsource dataset in order to then fit a estimator on the source dataset,\nwhile taking those weights into account, so that the fitted estimator is\nwell suited to predicting labels from points drawn from the target distribution.\n\nReweighting methods implemented and illustrated are the following:\n  * `Density Reweighting<Illustration of the Density Reweighting method>`\n  * `Gaussian Reweighting<Illustration of the Gaussian reweighting method>`\n  * `Discr. Reweighting<Illustration of the Discr. reweighting method>`\n  * `KLIEPReweight<Illustration of the KLIEPReweight method>`\n  * `Nearest Neighbor reweighting<Illustration of the Nearest Neighbor\n    reweighting method>`\n  * `Kernel Mean Matching<Illustration of the Kernel Mean Matching method>`\n\nFor more details, look at [3].\n\n.. [3] [Sugiyama et al., 2008] Sugiyama, M., Suzuki, T., Nakajima, S., Kashima, H.,\n       von B\u00fcnau, P., and Kawanabe, M. (2008). Direct importance estimation for\n       covariate shift adaptation. Annals of the Institute of Statistical\n       Mathematics, 60(4):699\u2013746.\n       https://www.ism.ac.jp/editsec/aism/pdf/060_4_0699.pdf\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "base_classifier = LogisticRegression().set_fit_request(sample_weight=True)\n\nprint(f\"Will be using {base_classifier} as base classifier\", end=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## We generate our 2D dataset with 2 classes\n\nWe generate a simple 2D dataset with covariate shift\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42\n\nX, y, sample_domain = make_shifted_datasets(\n    n_samples_source=20, n_samples_target=20, noise=0.1, random_state=RANDOM_SEED\n)\n\nXs, Xt, ys, yt = source_target_split(X, y, sample_domain=sample_domain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot of the dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x_min, x_max = -2.5, 4.5\ny_min, y_max = -1.5, 4.5\n\n\nfigsize = (8, 4)\nfigure, axes = plt.subplots(1, 2, figsize=figsize)\n\ncm = plt.cm.RdBu\ncolormap = ListedColormap([\"#FFA056\", \"#6C4C7C\"])\nax = axes[0]\nax.set_title(\"Source data\")\n# Plot the source points:\nax.scatter(Xs[:, 0], Xs[:, 1], c=ys, cmap=colormap, alpha=0.7, s=[25])\n\nax.set_xticks(()), ax.set_yticks(())\nax.set_xlim(x_min, x_max), ax.set_ylim(y_min, y_max)\n\nax = axes[1]\n\nax.set_title(\"Target data\")\n# Plot the target points:\nax.scatter(Xt[:, 0], Xt[:, 1], c=ys, cmap=colormap, alpha=0.1, s=[25])\nax.scatter(Xt[:, 0], Xt[:, 1], c=yt, cmap=colormap, alpha=0.7, s=[25])\nfigure.suptitle(\"Plot of the dataset\", fontsize=16, y=1)\nax.set_xticks(()), ax.set_yticks(())\nax.set_xlim(x_min, x_max), ax.set_ylim(y_min, y_max)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Illustration of the problem with no domain adaptation\n\nWhen not using domain adaptation, the classifier won't train on\ndata that is distributed as the target sample domain, it will thus\nnot be performing optimaly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We create a dict to store scores:\nscores_dict = {}\n\n\ndef plot_weights_and_classifier(\n    clf,\n    weights,\n    name=\"Without DA\",\n    suptitle=None,\n):\n    if suptitle is None:\n        suptitle = f\"Illustration of the {name} method\"\n    figure, axes = plt.subplots(1, 2, figsize=figsize)\n    ax = axes[1]\n    score = clf.score(Xt, yt)\n    DecisionBoundaryDisplay.from_estimator(\n        clf,\n        Xs,\n        cmap=ListedColormap([\"w\", \"k\"]),\n        alpha=1,\n        ax=ax,\n        eps=0.5,\n        response_method=\"predict\",\n        plot_method=\"contour\",\n    )\n\n    size = 5 + 10 * weights\n\n    # Plot the target points:\n    ax.scatter(\n        Xt[:, 0],\n        Xt[:, 1],\n        c=yt,\n        cmap=colormap,\n        alpha=0.7,\n        s=[25],\n    )\n\n    ax.set_xticks(()), ax.set_yticks(())\n    ax.set_xlim(x_min, x_max), ax.set_ylim(y_min, y_max)\n    ax.set_title(\"Accuracy on target\", fontsize=12)\n    ax.text(\n        x_max - 0.3,\n        y_min + 0.3,\n        (\"%.2f\" % score).lstrip(\"0\"),\n        size=15,\n        horizontalalignment=\"right\",\n    )\n    scores_dict[name] = score\n\n    ax = axes[0]\n\n    # Plot the source points:\n    ax.scatter(Xs[:, 0], Xs[:, 1], c=ys, cmap=colormap, alpha=0.7, s=size)\n\n    DecisionBoundaryDisplay.from_estimator(\n        clf,\n        Xs,\n        cmap=ListedColormap([\"w\", \"k\"]),\n        alpha=1,\n        ax=ax,\n        eps=0.5,\n        response_method=\"predict\",\n        plot_method=\"contour\",\n    )\n\n    ax.set_xticks(()), ax.set_yticks(())\n    ax.set_xlim(x_min, x_max), ax.set_ylim(y_min, y_max)\n    if name != \"Without DA\":\n        ax.set_title(\"Training with reweighted data\", fontsize=12)\n    else:\n        ax.set_title(\"Training data\", fontsize=12)\n    figure.suptitle(suptitle, fontsize=16, y=1)\n\n\nclf = base_classifier\nclf.fit(Xs, ys)\nplot_weights_and_classifier(\n    base_classifier,\n    name=\"Without DA\",\n    weights=np.array([2] * Xs.shape[0]),\n    suptitle=\"Illustration of the classifier with no DA\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Illustration of the Density Reweighting method\n\nThis method is trying to compute the optimal weights as a ratio of two probability\nfunctions, by default, it is the ratio of two kernel densities estimations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We define our classifier, `clf` is a da pipeline\nclf = DensityReweight(\n    base_estimator=base_classifier,\n    weight_estimator=KernelDensity(bandwidth=0.5),\n)\nclf.fit(X, y, sample_domain=sample_domain)\n\n# We get the weights:\n\n# we first get the adapter which is estimating the weights\nweight_estimator = clf[0].base_estimator_\nidx = extract_source_indices(sample_domain)\nweights = weight_estimator.adapt(X, sample_domain=sample_domain).sample_weight[idx]\n\nplot_weights_and_classifier(clf, weights=weights, name=\"Density Reweighting\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Illustration of the Gaussian reweighting method\nThis method tries to approximate the optimal weights by assuming that the data are\nnormally distributed, and thus approximating the probability functions for both source\nand target set, and setting the weight to be the ratio of the two.\n\nSee [1] for details.\n\n.. [1]  Hidetoshi Shimodaira. Improving predictive inference under\n        covariate shift by weighting the log-likelihood function.\n        In Journal of Statistical Planning and Inference, 2000.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We define our classifier, `clf` is a da pipeline\nclf = GaussianReweight(base_classifier)\nclf.fit(X, y, sample_domain=sample_domain)\n# We get the weights\nweight_estimator = clf[0].base_estimator_\nidx = extract_source_indices(sample_domain)\nweights = weight_estimator.adapt(X, sample_domain=sample_domain).sample_weight[idx]\n\nplot_weights_and_classifier(clf, weights=weights, name=\"Gaussian Reweighting\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Illustration of the Discr. reweighting method\n\nThis estimator derive a class of predictive densities by weighting the source samples\nwhen trying to maximize the log-likelihood function. Such approach is effective in\ncases of covariate shift.\n\nSee [1] for details.\n\n.. [1]    Hidetoshi Shimodaira. Improving predictive inference under\n          covariate shift by weighting the log-likelihood function.\n          In Journal of Statistical Planning and Inference, 2000.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We define our classifier, `clf` is a da pipeline\nclf = DiscriminatorReweight(base_classifier)\nclf.fit(X, y, sample_domain=sample_domain)\n\n# We get the weights:\n\n# we first get the adapter which is estimating the weights\nweight_estimator = clf[0].base_estimator_\nidx = extract_source_indices(sample_domain)\nweights = weight_estimator.adapt(X, sample_domain=sample_domain).sample_weight[idx]\n\nplot_weights_and_classifier(clf, weights=weights, name=\"Discr. Reweighting\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Illustration of the KLIEPReweight method\n\nThe idea of KLIEPReweight is to find an importance estimate $w(x)$ such that\nthe Kullback-Leibler (KL) divergence between the source input density\n$p_{source}(x)$ to its estimate $p_{target}(x) = w(x)p_{source}(x)$\nis minimized.\n\nSee [3] for details.\n\n.. [3] Masashi Sugiyama et. al. Direct Importance Estimation with Model Selection\n       and Its Application to Covariate Shift Adaptation.\n       In NeurIPS, 2007.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We define our classifier, `clf` is a da pipeline\nclf = KLIEPReweight(\n    LogisticRegression().set_fit_request(sample_weight=True), gamma=[1, 0.1, 0.001]\n)\nclf.fit(X, y, sample_domain=sample_domain)\n\n# We get the weights:\n\n# we first get the adapter which is estimating the weights\nweight_estimator = clf[0].base_estimator_\nidx = extract_source_indices(sample_domain)\nweights = weight_estimator.adapt(X, sample_domain=sample_domain).sample_weight[idx]\n\nplot_weights_and_classifier(clf, weights=weights, name=\"KLIEPReweight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Illustration of the Nearest Neighbor reweighting method\n.. _Nearest Neighbor reweighting\n\nThis method estimate weight of a point in the source dataset by\ncounting the number of points in the target set that are closer to\nit than any other points from the source dataset.\n\nSee [24] for details.\n\n.. [24] Loog, M. (2012).\n       Nearest neighbor-based importance weighting.\n       In 2012 IEEE International Workshop on Machine\n       Learning for Signal Processing, pages 1\u20136. IEEE\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We define our classifier, `clf` is a da pipeline\nclf = NearestNeighborReweight(base_classifier, laplace_smoothing=True)\nclf.fit(X, y, sample_domain=sample_domain)\n\n# We get the weights:\n\n# we first get the adapter which is estimating the weights\nweight_estimator = clf[0].base_estimator_\nidx = extract_source_indices(sample_domain)\nweights = weight_estimator.adapt(X, sample_domain=sample_domain).sample_weight[idx]\n\nplot_weights_and_classifier(clf, weights=weights, name=\"1NN Reweighting\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Illustration of the Kernel Mean Matching method\n.. _Kernel Mean Matching\n\nThis example illustrates the use of KMMReweight method [6] to correct covariate-shift.\nThis methods works without any estimation of the assumption, by matching distribution\nbetween training and testing sets in feature space.\n\nSee [25] for details.\n\n.. [25] J. Huang, A. Gretton, K. Borgwardt, B. Sch\u00f6lkopf and A. J. Smola.\n       Correcting sample selection bias by unlabeled data. In NIPS, 2007.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We define our classifier, `clf` is a da pipeline\nclf = KMMReweight(base_classifier, gamma=10.0, max_iter=1000, smooth_weights=False)\nclf.fit(X, y, sample_domain=sample_domain)\n\n# We get the weights:\n\n# we first get the adapter which is estimating the weights\nweight_estimator = clf[0].base_estimator_\nidx = extract_source_indices(sample_domain)\nweights = weight_estimator.adapt(X, sample_domain=sample_domain).sample_weight[idx]\n\nplot_weights_and_classifier(\n    clf,\n    weights=weights,\n    name=\"Kernel Mean Matching\",\n    suptitle=\"Illustration of KMMReweight without weights smoothing\",\n)\n\n# We define our classifier, `clf` is a da pipeline\nclf = KMMReweight(base_classifier, gamma=10.0, max_iter=1000, smooth_weights=True)\nclf.fit(X, y, sample_domain=sample_domain)\n\n# We get the weights:\n\n# we first get the adapter which is estimating the weights\nweight_estimator = clf[0].base_estimator_\nidx = extract_source_indices(sample_domain)\nweights = weight_estimator.adapt(X, sample_domain=sample_domain).sample_weight[idx]\n\nplot_weights_and_classifier(\n    clf,\n    weights=weights,\n    name=\"Kernel Mean Matching\",\n    suptitle=\"Illustration of KMMReweight with weights smoothing\",\n)\n\n# We define our classifier, `clf` is a da pipeline\nclf = KMMReweight(\n    base_classifier,\n    gamma=10.0,\n    max_iter=1000,\n    smooth_weights=True,\n    solver=\"frank-wolfe\",\n)\nclf.fit(X, y, sample_domain=sample_domain)\n\n# We get the weights:\n\n# we first get the adapter which is estimating the weights\nweight_estimator = clf[0].base_estimator_\nidx = extract_source_indices(sample_domain)\nweights = weight_estimator.adapt(X, sample_domain=sample_domain).sample_weight[idx]\n\nplot_weights_and_classifier(\n    clf,\n    weights=weights,\n    name=\"Kernel Mean Matching\",\n    suptitle=\"Illustration of KMMReweight with Frank-Wolfe solver\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison of score between reweighting methods:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def print_scores_as_table(scores):\n    keys = list(scores.keys())\n    lengths = [len(k) for k in keys]\n    max_lenght = max(lengths)\n    for k in keys:\n        print(f\"{k}{' '*(max_lenght - len(k))} | \", end=\"\")\n        print(f\"{scores[k]*100}{' '*(6-len(str(scores[k]*100)))}%\")\n\n\nprint_scores_as_table(scores_dict)\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}