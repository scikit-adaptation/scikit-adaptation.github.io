{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Label Propagation methods\n\nThis example shows how to use how to use label propagation methods to perform\ndomain adaptation. This is done by propagating labels from the source domain to\nthe target domain using the OT plan. This was proposed originally in [28]_ for\nsemi-supervised learning but can be used for DA.\n\nWe illustrate the method on a simple regression and classification conditional shift\ndataset. We train a simple Kernel Ridge Regression (KRR) and Logistic Regression\non the source domain and evaluate their performance on the source and target\ndomain. We then train the same models with the label propagation method and\nevaluate their performance on the source and target domain.\n\n.. [28] Solomon, J., Rustamov, R., Guibas, L., & Butscher, A. (2014, January).\n     Wasserstein propagation for semi-supervised learning. In International\n     Conference on Machine Learning (pp. 306-314). PMLR.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Remi Flamary\n#\n# License: BSD 3-Clause\n# sphinx_gallery_thumbnail_number = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import SVC\n\nfrom skada import (\n    JCPOTLabelPropAdapter,\n    OTLabelPropAdapter,\n    make_da_pipeline,\n    source_target_split,\n)\nfrom skada.datasets import make_shifted_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate conditional shift regression dataset and plot it\n\nWe generate a simple 2D conditional shift dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y, sample_domain = make_shifted_datasets(\n    n_samples_source=20,\n    n_samples_target=20,\n    shift=\"conditional_shift\",\n    noise=0.3,\n    label=\"regression\",\n    random_state=42,\n)\n\ny = (y - y.mean()) / y.std()\n\nXs, Xt, ys, yt = source_target_split(X, y, sample_domain=sample_domain)\n\n\nplt.figure(1, (10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(Xs[:, 0], Xs[:, 1], c=ys, label=\"Source\")\nplt.title(\"Source data\")\nax = plt.axis()\n\nplt.subplot(1, 2, 2)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yt, label=\"Target\")\nplt.title(\"Target data\")\nplt.axis(ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train a regressor on source data\n\nWe train a simple Kernel Ridge Regression (KRR) on the source domain and\nevaluate its performance on the source and target domain. Performance is\nmuch lower on the target domain due to the shift. We also plot the decision\nboundary learned by the KRR.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = KernelRidge(kernel=\"rbf\", alpha=0.5)\nclf.fit(Xs, ys)\n\n# Compute accuracy on source and target\nys_pred = clf.predict(Xs)\nyt_pred = clf.predict(Xt)\n\nmse_s = mean_squared_error(ys, ys_pred)\nmse_t = mean_squared_error(yt, yt_pred)\n\nprint(f\"MSE on source: {mse_s:.2f}\")\nprint(f\"MSE on target: {mse_t:.2f}\")\n\nXX, YY = np.meshgrid(np.linspace(ax[0], ax[1], 100), np.linspace(ax[2], ax[3], 100))\nZ = clf.predict(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)\n\n\nplt.figure(2, (10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(Xs[:, 0], Xs[:, 1], c=ys, label=\"Prediction\")\nplt.imshow(Z, extent=(ax[0], ax[1], ax[2], ax[3]), origin=\"lower\", alpha=0.5)\nplt.title(f\"KRR Prediction on source (MSE={mse_s:.2f})\")\nplt.axis(ax)\n\nplt.subplot(1, 2, 2)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yt, label=\"Prediction\")\nplt.imshow(Z, extent=(ax[0], ax[1], ax[2], ax[3]), origin=\"lower\", alpha=0.5)\nplt.title(f\"KRR Prediction on target (MSE={mse_t:.2f})\")\nplt.axis(ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the full Labe Propagation model\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = make_da_pipeline(OTLabelPropAdapter(), KernelRidge(kernel=\"rbf\", alpha=0.5))\nclf.fit(X, y, sample_domain=sample_domain)\n\nys_pred = clf.predict(Xs)\nyt_pred = clf.predict(Xt)\n\nmse_s = mean_squared_error(ys, ys_pred)\nmse_t = mean_squared_error(yt, yt_pred)\n\nZjdot = clf.predict(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)\n\nprint(f\"LabelProp MSE on source: {mse_s:.2f}\")\nprint(f\"LabelProp MSE on target: {mse_t:.2f}\")\n\nplt.figure(3, (10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(Xs[:, 0], Xs[:, 1], c=ys, label=\"Prediction\")\nplt.imshow(Zjdot, extent=(ax[0], ax[1], ax[2], ax[3]), origin=\"lower\", alpha=0.5)\nplt.title(f\"LabelProp Prediction on source (MSE={mse_s:.2f})\")\nplt.axis(ax)\n\nplt.subplot(1, 2, 2)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yt, label=\"Prediction\")\nplt.imshow(Zjdot, extent=(ax[0], ax[1], ax[2], ax[3]), origin=\"lower\", alpha=0.5)\nplt.title(f\"LabelProp Prediction on target (MSE={mse_t:.2f})\")\nplt.axis(ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Illustration of the propagated labels\n\nWe illustrate the propagated labels on the target domain. We can see that the\nlabels are propagated from the source domain to the target domain.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lp = OTLabelPropAdapter()\n\nyh = lp.fit_transform(X, y, sample_domain=sample_domain)[1]\n\nyht = yh[sample_domain < 0]  #\n\nplt.figure(1, (10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(Xs[:, 0], Xs[:, 1], c=ys, label=\"Source\")\nplt.scatter(Xt[:, 0], Xt[:, 1], c=\"gray\", label=\"Target\")\nplt.legend()\n\nplt.title(\"Source and Target data\")\nax = plt.axis()\n\nplt.subplot(1, 2, 2)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yht, label=\"Target\")\n\n\nplt.title(\"Propagated labels data\")\nplt.axis(ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate conditional shift classification dataset and plot it\n\nWe generate a simple 2D conditional shift dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y, sample_domain = make_shifted_datasets(\n    n_samples_source=20,\n    n_samples_target=20,\n    shift=\"conditional_shift\",\n    noise=0.2,\n    label=\"multiclass\",\n    random_state=42,\n)\n\n\nXs, Xt, ys, yt = source_target_split(X, y, sample_domain=sample_domain)\n\n\nplt.figure(5, (10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(Xs[:, 0], Xs[:, 1], c=ys, cmap=\"tab10\", vmax=9, label=\"Source\")\nplt.title(\"Source data\")\nax = plt.axis()\n\nplt.subplot(1, 2, 2)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yt, cmap=\"tab10\", vmax=9, label=\"Target\")\nplt.title(\"Target data\")\nplt.axis(ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train a classifier on source data\n\nWe train a simple SVC classifier on the source domain and evaluate its\nperformance on the source and target domain. Performance is much lower on\nthe target domain due to the shift. We also plot the decision boundary\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression()\nclf.fit(Xs, ys)\n\n# Compute accuracy on source and target\nys_pred = clf.predict(Xs)\nyt_pred = clf.predict(Xt)\n\nacc_s = (ys_pred == ys).mean()\nacc_t = (yt_pred == yt).mean()\n\nprint(f\"Accuracy on source: {acc_s:.2f}\")\nprint(f\"Accuracy on target: {acc_t:.2f}\")\n\nXX, YY = np.meshgrid(np.linspace(ax[0], ax[1], 100), np.linspace(ax[2], ax[3], 100))\nZ = clf.predict(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)\n\n\nplt.figure(6, (10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(Xs[:, 0], Xs[:, 1], c=ys, cmap=\"tab10\", vmax=9, label=\"Prediction\")\nplt.imshow(\n    Z,\n    extent=(ax[0], ax[1], ax[2], ax[3]),\n    origin=\"lower\",\n    alpha=0.5,\n    cmap=\"tab10\",\n    vmax=9,\n)\nplt.title(f\"LogReg Prediction on source (ACC={acc_s:.2f})\")\n\nplt.subplot(1, 2, 2)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yt, cmap=\"tab10\", vmax=9, label=\"Prediction\")\nplt.imshow(\n    Z,\n    extent=(ax[0], ax[1], ax[2], ax[3]),\n    origin=\"lower\",\n    alpha=0.5,\n    cmap=\"tab10\",\n    vmax=9,\n)\nplt.title(f\"LogReg Prediction on target (ACC={acc_t:.2f})\")\nplt.axis(ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train with LabelProp + classifier\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = make_da_pipeline(OTLabelPropAdapter(), LogisticRegression())\nclf.fit(X, y, sample_domain=sample_domain)\n\n\nys_pred = clf.predict(Xs)\nyt_pred = clf.predict(Xt)\n\nacc_s = (ys_pred == ys).mean()\nacc_t = (yt_pred == yt).mean()\n\nprint(f\"LabelProp Accuracy on source: {acc_s:.2f}\")\nprint(f\"LabelProp Accuracy on target: {acc_t:.2f}\")\n\nXX, YY = np.meshgrid(np.linspace(ax[0], ax[1], 100), np.linspace(ax[2], ax[3], 100))\nZ = clf.predict(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)\n\n\nplt.figure(7, (10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(Xs[:, 0], Xs[:, 1], c=ys, cmap=\"tab10\", vmax=9, label=\"Prediction\")\nplt.imshow(\n    Z,\n    extent=(ax[0], ax[1], ax[2], ax[3]),\n    origin=\"lower\",\n    alpha=0.5,\n    cmap=\"tab10\",\n    vmax=9,\n)\nplt.title(f\"LabelProp reglog on source (ACC={acc_s:.2f})\")\n\nplt.subplot(1, 2, 2)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yt, cmap=\"tab10\", vmax=9, label=\"Prediction\")\nplt.imshow(\n    Z,\n    extent=(ax[0], ax[1], ax[2], ax[3]),\n    origin=\"lower\",\n    alpha=0.5,\n    cmap=\"tab10\",\n    vmax=9,\n)\nplt.title(f\"LabelProp reglog on target (ACC={acc_t:.2f})\")\nplt.axis(ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Illustration of the propagated labels\n\nWe illustrate the propagated labels on the target domain. We can see that the\nlabels are propagated from the source domain to the target domain.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lp = OTLabelPropAdapter()\n\nyh = lp.fit_transform(X, y, sample_domain=sample_domain)[1]\n\nyht = yh[sample_domain < 0]\n\nplt.figure(1, (10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(Xs[:, 0], Xs[:, 1], c=ys, cmap=\"tab10\", vmax=9, label=\"Source\")\nplt.scatter(Xt[:, 0], Xt[:, 1], c=\"gray\", label=\"Target\")\nplt.legend()\nplt.title(\"Source and Target data\")\nax = plt.axis()\n\nplt.subplot(1, 2, 2)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yht, cmap=\"tab10\", vmax=9, label=\"Target\")\n\n\nplt.title(\"Propagated labels data\")\nplt.axis(ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate classification classification dataset and plot it\n\nWe generate a simple 2D target shift dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y, sample_domain = make_shifted_datasets(\n    n_samples_source=20,\n    n_samples_target=20,\n    shift=\"target_shift\",\n    noise=0.2,\n    random_state=42,\n)\n\n\nXs, Xt, ys, yt = source_target_split(X, y, sample_domain=sample_domain)\n\n\nplt.figure(5, (10, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(Xs[:, 0], Xs[:, 1], c=ys, cmap=\"tab10\", vmax=9, label=\"Source\")\nplt.title(\"Source data\")\nax = plt.axis()\n\nplt.subplot(1, 2, 2)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yt, cmap=\"tab10\", vmax=9, label=\"Target\")\nplt.title(\"Target data\")\nplt.axis(ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train with LabelProp and JCPOT + classifier\n\nOn this target shift dataset, we can see that the label propagation method\ndoes not work well because it finds correspondences between the source and\ntarget samples with different classes. In this case JCPOT is more robust\nto this kind of shift because it estimates the class proportions in the target.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = make_da_pipeline(OTLabelPropAdapter(), SVC())\nclf.fit(X, y, sample_domain=sample_domain)\n\nclf_jcpot = make_da_pipeline(JCPOTLabelPropAdapter(reg=0.1), SVC())\nclf_jcpot.fit(X, y, sample_domain=sample_domain)\n\n\nyt_pred = clf.predict(Xt)\nacc_t = (yt_pred == yt).mean()\n\nprint(f\"LabelProp Accuracy on target: {acc_t:.2f}\")\n\n\nyt_pred = clf_jcpot.predict(Xt)\nacc_s_jcpot = (yt_pred == yt).mean()\n\nprint(f\"JCPOT Accuracy on target: {acc_s_jcpot:.2f}\")\n\nXX, YY = np.meshgrid(np.linspace(ax[0], ax[1], 100), np.linspace(ax[2], ax[3], 100))\nZ = clf.predict(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)\nZ_jcpot = clf_jcpot.predict(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)\n\n\nplt.figure(7, (10, 5))\n\n\nplt.subplot(1, 2, 1)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yt, cmap=\"tab10\", vmax=9, label=\"Prediction\")\nplt.imshow(\n    Z,\n    extent=(ax[0], ax[1], ax[2], ax[3]),\n    origin=\"lower\",\n    alpha=0.5,\n    cmap=\"tab10\",\n    vmax=9,\n)\nplt.title(f\"LabelProp reglog on target (ACC={acc_t:.2f})\")\nplt.axis(ax)\n\nplt.subplot(1, 2, 2)\nplt.scatter(Xt[:, 0], Xt[:, 1], c=yt, cmap=\"tab10\", vmax=9, label=\"Prediction\")\nplt.imshow(\n    Z_jcpot,\n    extent=(ax[0], ax[1], ax[2], ax[3]),\n    origin=\"lower\",\n    alpha=0.5,\n    cmap=\"tab10\",\n    vmax=9,\n)\nplt.title(f\"JCPOT reglog on target (ACC={acc_s_jcpot:.2f})\")\nplt.axis(ax)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}