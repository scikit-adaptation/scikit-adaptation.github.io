{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Subspace method example on subspace shift dataset\n\nAn example of the subspace methods on a dataset subject\nto subspace shift\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author:   Ruben Bueno <ruben.bueno@polytechnique.edu>\n#           Antoine Collas <contact@antoinecollas.fr>\n#           Oleksii Kachaiev <kachayev@gmail.com>\n#\n# License: BSD 3-Clause\n# sphinx_gallery_thumbnail_number = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.svm import SVC\n\nfrom skada import (\n    SubspaceAlignment,\n    TransferComponentAnalysis,\n    TransferJointMatching,\n    TransferSubspaceLearning,\n    source_target_split,\n)\nfrom skada.datasets import make_shifted_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The subspaces methods\n\nSupspace methods are used in unsupervised domain adaptation.\nIn this case, we have labeled data for the source domain but not for the target\ndomain.\nThe goal of subspace methods is to project data from a d-dimensional space\ninto a lower-dimensional space with d' < d.\nSubspace methods are particularly effective when dealing with subspace shift,\nwhere the source and target data have the same distributions when projected onto a\nsubspace.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The Subspace methods implemented and illustrated are the following:\n#   * :ref:`Subspace Alignment<Illustration of the Subspace Alignment method>`\n#   * :ref:`Transfer Component Analysis<Illustration of the Transfer Component\n#     Analysis method>`\n#   * :ref:`Transfer Joint Matching<Illustration of the Transfer Joint Matching method>`\n\n\nbase_classifier = SVC()\n\nprint(f\"Will be using {base_classifier} as base classifier\", end=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## We generate our 2D dataset with 2 classes\n\nWe generate a simple 2D dataset with subspace shift.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42\n\ndataset = make_shifted_datasets(\n    n_samples_source=20,\n    n_samples_target=20,\n    noise=0.1,\n    random_state=RANDOM_SEED,\n    shift=\"subspace\",\n    return_dataset=True,\n)\n\nX_train, y_train, sample_domain_train = dataset.pack(\n    as_sources=[\"s\"], as_targets=[\"t\"], mask_target_labels=True\n)\nX, y, sample_domain = dataset.pack(\n    as_sources=[\"s\"],\n    as_targets=[\"t\"],\n    mask_target_labels=False,\n)\nXs, Xt, ys, yt = source_target_split(X, y, sample_domain=sample_domain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot of the dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x_min, x_max = -2.4, 2.4\ny_min, y_max = -2.4, 2.4\ntarget_marker = \"v\"\nsource_marker = \"^\"\n\nfigsize = (8, 4)\nfigure, axes = plt.subplots(1, 2, figsize=figsize)\n\ncm = plt.cm.RdBu\ncolormap = ListedColormap([\"#FFA056\", \"#6C4C7C\"])\nax = axes[0]\nax.set_title(\"Source data\")\n# Plot the source points:\nax.scatter(\n    Xs[:, 0], Xs[:, 1], c=ys, cmap=colormap, alpha=0.7, s=[15], marker=source_marker\n)\n\nax.set_xticks(()), ax.set_yticks(())\nax.set_xlim(x_min, x_max), ax.set_ylim(y_min, y_max)\n\nax = axes[1]\n\nax.set_title(\"Target data\")\n# Plot the target points:\nax.scatter(\n    Xt[:, 0], Xt[:, 1], c=yt, cmap=colormap, alpha=0.7, s=[15], marker=target_marker\n)\nfigure.suptitle(\"Plot of the dataset\", fontsize=16, y=1)\nax.set_xticks(()), ax.set_yticks(())\nax.set_xlim(x_min, x_max), ax.set_ylim(y_min, y_max)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Illustration of the problem with no domain adaptation\n\nWhen not using domain adaptation, the classifier won't train on\ndata that is distributed as the target sample domain, it will thus\nnot be performing optimaly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We create a dict to store scores:\nscores_dict = {}\n\n\ndef plot_subspace_and_classifier(\n    clf,\n    name=\"Without DA\",\n    suptitle=None,\n):\n    size = 16\n\n    if suptitle is None:\n        suptitle = f\"Illustration of the {name} method\"\n    figure, axes = plt.subplots(1, 3, figsize=(figsize[0] * 1.5, figsize[1]))\n    ax = axes[2]\n    score = clf.score(Xt, yt)\n    DecisionBoundaryDisplay.from_estimator(\n        clf,\n        Xs,\n        cmap=colormap,\n        alpha=0.1,\n        ax=ax,\n        eps=0.5,\n        response_method=\"predict\",\n    )\n\n    # Plot the target points:\n    ax.scatter(\n        Xt[:, 0], Xt[:, 1], c=yt, cmap=colormap, alpha=0.7, s=[15], marker=target_marker\n    )\n\n    ax.set_xticks(()), ax.set_yticks(())\n    ax.set_xlim(x_min, x_max), ax.set_ylim(y_min, y_max)\n    ax.set_title(\"Accuracy on target\", fontsize=12)\n    ax.text(\n        x_max - 0.3,\n        y_min + 0.3,\n        (\"%.2f\" % score).lstrip(\"0\"),\n        size=15,\n        horizontalalignment=\"right\",\n    )\n    scores_dict[name] = score\n    if name != \"Without DA\":\n        subspace_estimator = clf.steps[-2][1].get_estimator()\n        clf_on_subspace = clf.steps[-1][1].get_estimator()\n\n        ax = axes[0]\n\n        # Plot the source points:\n        Xs_subspace = subspace_estimator.transform(\n            Xs,\n            # mark all samples as sources\n            sample_domain=np.ones(Xs.shape[0]),\n            allow_source=True,\n        )\n        Xt_subspace = subspace_estimator.transform(Xt)\n        ax.scatter(\n            Xs_subspace,\n            [1] * Xs.shape[0],\n            c=ys,\n            cmap=colormap,\n            alpha=0.5,\n            s=size,\n            marker=source_marker,\n        )\n        ax.scatter(\n            Xt_subspace,\n            [-1] * Xt.shape[0],\n            c=yt,\n            cmap=colormap,\n            alpha=0.5,\n            s=size,\n            marker=target_marker,\n        )\n\n        ax.set_xticks(()), ax.set_yticks(())\n        ax.set_ylim(-50, 50)\n        ax.set_title(\"Full dataset projected on the subspace\", fontsize=12)\n\n        ax = axes[1]\n        Xt_adapted = subspace_estimator.transform(Xt)\n        ax.scatter(\n            Xt_adapted,\n            [0] * Xt.shape[0],\n            c=yt,\n            cmap=colormap,\n            alpha=0.5,\n            s=size,\n            marker=target_marker,\n        )\n        m, M = min(Xt_adapted), max(Xt_adapted)\n        x_ = list(np.linspace(m - abs(m) / 4, M + abs(M) / 4, 100).reshape(-1, 1))\n        y_ = list(clf_on_subspace.predict(x_))\n        ax.scatter(\n            x_ * 100,\n            [j // 100 - 50 for j in range(100 * 100)],\n            c=y_ * 100,\n            cmap=colormap,\n            alpha=0.02,\n            s=size,\n        )\n        ax.set_ylim(-50, 50)\n        ax.set_title(\"Accuracy on target projected on the subspace\", fontsize=12)\n    else:\n        ax = axes[0]\n        ax.scatter(\n            Xs[:, 0],\n            Xs[:, 1],\n            c=ys,\n            cmap=colormap,\n            alpha=0.5,\n            s=size,\n            marker=source_marker,\n        )\n        ax.scatter(\n            Xt[:, 0],\n            Xt[:, 1],\n            c=yt,\n            cmap=colormap,\n            alpha=0.5,\n            s=size,\n            marker=target_marker,\n        )\n\n        ax.set_xticks(()), ax.set_yticks(())\n        ax.set_xlim(x_min, x_max), ax.set_ylim(y_min, y_max)\n        ax.set_title(\"Full dataset\", fontsize=12)\n\n        ax = axes[1]\n        score = clf.score(Xs, ys)\n        DecisionBoundaryDisplay.from_estimator(\n            clf,\n            Xs,\n            cmap=colormap,\n            alpha=0.1,\n            ax=ax,\n            eps=0.5,\n            response_method=\"predict\",\n        )\n\n        # Plot the source points:\n        ax.scatter(\n            Xs[:, 0],\n            Xs[:, 1],\n            c=ys,\n            cmap=colormap,\n            alpha=0.7,\n            s=[15],\n            marker=source_marker,\n        )\n\n        ax.set_xticks(()), ax.set_yticks(())\n        ax.set_xlim(x_min, x_max), ax.set_ylim(y_min, y_max)\n        ax.set_title(\"Accuracy on source\", fontsize=12)\n        ax.text(\n            x_max - 0.3,\n            y_min + 0.3,\n            (\"%.2f\" % score).lstrip(\"0\"),\n            size=15,\n            horizontalalignment=\"right\",\n        )\n\n    figure.suptitle(suptitle, fontsize=16, y=1)\n    figure.tight_layout()\n\n\nclf = base_classifier\nclf.fit(Xs, ys)\nplot_subspace_and_classifier(\n    base_classifier, \"Without DA\", suptitle=\"Illustration of the classifier with no DA\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Illustration of the Subspace Alignment method\n\nAs we assume that the  source and target domains are represented\nby subspaces described by eigenvectors;\nThis method seeks a domain adaptation solution by learning a mapping\nfunction which aligns the source subspace with the target one.\n\nSee [8] for details:\n\n.. [8] Basura Fernando et. al. Unsupervised Visual\n       Domain Adaptation Using Subspace Alignment.\n       In IEEE International Conference on Computer Vision, 2013.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = SubspaceAlignment(base_classifier, n_components=1)\nclf.fit(X_train, y_train, sample_domain=sample_domain_train)\nplot_subspace_and_classifier(clf, \"Subspace Alignment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Illustration of the Transfer Component Analysis method\n\nThe goal of Transfer Component Analysis (TCA) is to learn some transfer\ncomponents across domains in a reproducing kernel Hilbert space using Maximum\nMean Discrepancy (MMD)\n\nSee [9] for details:\n\n.. [9] Sinno Jialin Pan et. al. Domain Adaptation via\n       Transfer Component Analysis. In IEEE Transactions\n       on Neural Networks, 2011.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = TransferComponentAnalysis(base_classifier, n_components=1, mu=2)\nclf.fit(X_train, y_train, sample_domain=sample_domain_train)\nplot_subspace_and_classifier(clf, \"TCA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Illustration of the Transfer Joint Matching method\n\nIn most of the previous works, we explored two learning strategies independently for\ndomain adaptation: feature matching and instance reweighting. Transfer Joint Matching\n(TJM) aims to use both, by adding a constant to tradeoff between the two.\n\nSee [26] for details:\n\n.. [26] Long et al., 2014] Long, M., Wang, J., Ding, G., Sun, J., and Yu, P. (2014).\n        Transfer joint matching for unsupervised domain adaptation. In IEEE Conference\n        on Computer Vision and Pattern Recognition (CVPR), pages 1410\u20131417.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = TransferJointMatching(base_classifier, tradeoff=0.1, n_components=1)\nclf.fit(X_train, y_train, sample_domain=sample_domain_train)\nplot_subspace_and_classifier(\n    clf,\n    \"TransferJointMatching with rbf kernel\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Illustration of the Transfer Subspace Learning method\n\nTransfer Subspace Learning (TSL) is a method that aims to learn a subspace using\nclassical loss functions (e.g. PCA, Fisher LDA) but regularized so that\nthe source and target data have the same distribution once projected on the subspace.\n\nSee [27] for details:\n\n.. [27]  [Si et al., 2010] Si, S., Tao, D. and Geng, B.\n          Bregman Divergence-Based Regularization\n          for Transfer Subspace Learning.\n          In IEEE Transactions on Knowledge and Data Engineering.\n          pages 929-942\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = TransferSubspaceLearning(base_classifier, n_components=1)\nclf.fit(X, y, sample_domain=sample_domain)\nplot_subspace_and_classifier(clf, \"TransferSubspaceLearning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison of score between subspace methods:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def print_scores_as_table(scores):\n    max_len = max(len(k) for k in scores.keys())\n    for k, v in scores.items():\n        print(f\"{k}{' '*(max_len - len(k))} | \", end=\"\")\n        print(f\"{v*100}{' '*(6-len(str(v*100)))}%\")\n\n\nprint_scores_as_table(scores_dict)\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}