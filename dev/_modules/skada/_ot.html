

<!DOCTYPE html>
<html class="writer-html5" lang="English" data-content_root="../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>skada._ot &mdash; SKADA : Scikit Adaptation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css?v=4ae1632d" />
      <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=d2d258e8" />
      <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=22974a12"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            SKADA
              <img src="../../_static/skada_logo_full_white.svg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../index.html">SKADA: SciKit Domain Adaptation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../auto_examples/plot_how_to_use_skada.html">How to use SKADA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html">Users Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../all.html">API and modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../auto_examples/index.html">Examples gallery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases.html">Release of SKADA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing to SKADA</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">SKADA</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Module code</a></li>
      <li class="breadcrumb-item active">skada._ot</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for skada._ot</h1><div class="highlight"><pre>
<span></span><span class="c1"># Author: Remi Flamary &lt;remi.flamary@polytechnique.edu&gt;</span>
<span class="c1">#</span>
<span class="c1"># License: BSD 3-Clause</span>


<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">ot</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">clone</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.metaestimators</span> <span class="kn">import</span> <span class="n">available_if</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.validation</span> <span class="kn">import</span> <span class="n">check_is_fitted</span>

<span class="kn">from</span> <span class="nn">._pipeline</span> <span class="kn">import</span> <span class="n">make_da_pipeline</span>
<span class="kn">from</span> <span class="nn">._utils</span> <span class="kn">import</span> <span class="n">Y_Type</span><span class="p">,</span> <span class="n">_find_y_type</span>
<span class="kn">from</span> <span class="nn">.base</span> <span class="kn">import</span> <span class="n">BaseAdapter</span><span class="p">,</span> <span class="n">DAEstimator</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">check_X_y_domain</span><span class="p">,</span> <span class="n">per_domain_split</span><span class="p">,</span> <span class="n">source_target_split</span>


<span class="k">def</span> <span class="nf">get_jdot_class_cost_matrix</span><span class="p">(</span><span class="n">Ys</span><span class="p">,</span> <span class="n">Xt</span><span class="p">,</span> <span class="n">estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;multinomial&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Cost matrix for joint distribution optimal transport classification problem.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    Ys : array-like of shape (n_samples,n_classes)</span>
<span class="sd">        Source domain labels one hot encoded.</span>
<span class="sd">    Xt : array-like of shape (m_samples, n_features)</span>
<span class="sd">        Target domain samples.</span>
<span class="sd">    estimator : object</span>
<span class="sd">        The already fitted estimator to be used for the classification task. This</span>
<span class="sd">        estimator should optimize a classification loss corresponding to the</span>
<span class="sd">        given metric and provide compatible predict method (decision_function of</span>
<span class="sd">        predict_proba). If None, a constant prediction is used.</span>
<span class="sd">    metric : str, default=&#39;multinomial&#39;</span>
<span class="sd">        The metric to use for the cost matrix. Can be &#39;multinomial&#39; for cross-entropy</span>
<span class="sd">        cost/ multinomial logistic regression or &#39;hinge&#39; for hinge cost (SVM/SVC).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    M : array-like of shape (n_samples, m_samples)</span>
<span class="sd">        The cost matrix.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    [10] N. Courty, R. Flamary, A. Habrard, A. Rakotomamonjy, Joint Distribution</span>
<span class="sd">         Optimal Transportation for Domain Adaptation, Neural Information Processing</span>
<span class="sd">         Systems (NIPS), 2017.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">Ys</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Xt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="o">*</span> <span class="mi">10</span>
        <span class="k">return</span> <span class="n">M</span>

    <span class="k">if</span> <span class="n">metric</span> <span class="o">==</span> <span class="s2">&quot;multinomial&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="s2">&quot;predict_log_proba&quot;</span><span class="p">):</span>
            <span class="n">Yt_pred</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict_log_proba</span><span class="p">(</span><span class="n">Xt</span><span class="p">)</span>
            <span class="n">M</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Ys</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">Yt_pred</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="s2">&quot;predict_proba&quot;</span><span class="p">):</span>
            <span class="n">Yt_pred</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">Xt</span><span class="p">)</span>
            <span class="n">M</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Ys</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Yt_pred</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">+</span> <span class="mf">1e-16</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Estimator must have predict_proba or predict_log_proba&quot;</span>
                <span class="s2">&quot; method for cce loss&quot;</span>
            <span class="p">)</span>

    <span class="k">elif</span> <span class="n">metric</span> <span class="o">==</span> <span class="s2">&quot;hinge&quot;</span><span class="p">:</span>
        <span class="n">Ys</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">Ys</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># make Y -1/1 for hinge loss</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="s2">&quot;decision_function&quot;</span><span class="p">):</span>
            <span class="n">Yt_pred</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">Xt</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">Yt_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">Yt_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">Yt_pred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">Ys</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">Yt_pred</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Estimator must have decision_function method for hinge loss&quot;</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unknown metric&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">M</span>


<span class="k">def</span> <span class="nf">get_data_jdot_class</span><span class="p">(</span><span class="n">Xt</span><span class="p">,</span> <span class="n">Yth</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">thr_weights</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get data for the joint distribution optimal transport classification problem.</span>

<span class="sd">    This function will repeat sample to allow for training on uncertain labels.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    Xt : array-like of shape (m_samples, n_features)</span>
<span class="sd">        Target domain samples.</span>
<span class="sd">    Yth : array-like of shape (n_samples,n_classes)</span>
<span class="sd">        Transported source domain labels one hot encoded.</span>
<span class="sd">    labels : array-like of shape (n_classes,)</span>
<span class="sd">        The labels of the classification problem.</span>
<span class="sd">    thr_weights : float, default=1e-6</span>
<span class="sd">        The relative threshold for the weights</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Xh : array-like of shape (n_samples, n_features)</span>
<span class="sd">        The transported source domain samples.</span>
<span class="sd">    yh : array-like of shape (n_samples,)</span>
<span class="sd">        The transported source domain labels.</span>
<span class="sd">    wh : array-like of shape (n_samples,)</span>
<span class="sd">        The transported source domain weights.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    [10] N. Courty, R. Flamary, A. Habrard, A. Rakotomamonjy, Joint Distribution</span>
<span class="sd">         Optimal Transportation for Domain Adaptation, Neural Information Processing</span>
<span class="sd">         Systems (NIPS), 2017.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">thr</span> <span class="o">=</span> <span class="n">thr_weights</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Yth</span><span class="p">)</span>

    <span class="n">Xh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">Xt</span><span class="p">,</span> <span class="n">Yth</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">yh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">Yth</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">wh</span> <span class="o">=</span> <span class="n">Yth</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

    <span class="c1"># remove samples with low weights</span>
    <span class="n">ind</span> <span class="o">=</span> <span class="n">wh</span> <span class="o">&gt;</span> <span class="n">thr</span>
    <span class="n">Xh</span> <span class="o">=</span> <span class="n">Xh</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
    <span class="n">yh</span> <span class="o">=</span> <span class="n">yh</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
    <span class="n">wh</span> <span class="o">=</span> <span class="n">wh</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">Xh</span><span class="p">,</span> <span class="n">yh</span><span class="p">,</span> <span class="n">wh</span>


<span class="k">def</span> <span class="nf">get_tgt_loss_jdot_class</span><span class="p">(</span><span class="n">Xh</span><span class="p">,</span> <span class="n">yh</span><span class="p">,</span> <span class="n">wh</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;multinomial&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get target loss for joint distribution optimal transport classification problem.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    Xh : array-like of shape (n_samples, n_features)</span>
<span class="sd">        The transported source domain samples.</span>
<span class="sd">    yh : array-like of shape (n_samples,)</span>
<span class="sd">        The transported source domain labels.</span>
<span class="sd">    wh : array-like of shape (n_samples,)</span>
<span class="sd">        The transported source domain weights.</span>
<span class="sd">    estimator : object</span>
<span class="sd">        The already fitted estimator to be used for the classification task. This</span>
<span class="sd">        estimator should optimize a classification loss corresponding to the</span>
<span class="sd">        given metric and provide compatible predict method (decision_function of</span>
<span class="sd">        predict_proba).</span>
<span class="sd">    metric : str, default=&#39;multinomial&#39;</span>
<span class="sd">        The metric to use for the cost matrix. Can be &#39;multinomial&#39; for cross-entropy</span>
<span class="sd">        cost/ multinomial logistic regression or &#39;hinge&#39; for hinge cost</span>
<span class="sd">        (SVM/SVC).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss : float</span>
<span class="sd">        The target labels losses.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    [10] N. Courty, R. Flamary, A. Habrard, A. Rakotomamonjy, Joint Distribution</span>
<span class="sd">         Optimal Transportation for Domain Adaptation, Neural Information Processing</span>
<span class="sd">         Systems (NIPS), 2017.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">metric</span> <span class="o">==</span> <span class="s2">&quot;multinomial&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="s2">&quot;predict_log_proba&quot;</span><span class="p">):</span>
            <span class="n">Yh_pred</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict_log_proba</span><span class="p">(</span><span class="n">Xh</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">yh</span> <span class="o">*</span> <span class="n">Yh_pred</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">wh</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="s2">&quot;predict_proba&quot;</span><span class="p">):</span>
            <span class="n">Yh_pred</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">Xh</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">yh</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Yh_pred</span> <span class="o">+</span> <span class="mf">1e-16</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">wh</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Estimator must have predict_proba or predict_log_proba method&quot;</span>
                <span class="s2">&quot; for multinomial loss&quot;</span>
            <span class="p">)</span>

    <span class="k">elif</span> <span class="n">metric</span> <span class="o">==</span> <span class="s2">&quot;hinge&quot;</span><span class="p">:</span>
        <span class="n">yh</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">yh</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># make Y -1/1 for hinge loss</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="s2">&quot;decision_function&quot;</span><span class="p">):</span>
            <span class="n">Yh_pred</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">Xh</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">Yh_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># handle binary classification</span>
                <span class="n">Yh_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">Yh_pred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">yh</span> <span class="o">*</span> <span class="n">Yh_pred</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">wh</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Estimator must have decision_function method for hinge loss&quot;</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unknown metric&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span>


<span class="k">def</span> <span class="nf">solve_jdot_regression</span><span class="p">(</span>
    <span class="n">base_estimator</span><span class="p">,</span>
    <span class="n">Xs</span><span class="p">,</span>
    <span class="n">ys</span><span class="p">,</span>
    <span class="n">Xt</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">ws</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">wt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">n_iter_max</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">tol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Solve the joint distribution optimal transport regression problem [10]</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This estimator assumes that the loss function optimized by the base</span>
<span class="sd">        estimator is the quadratic loss. For instance, the base estimator should</span>
<span class="sd">        optimize and L2 loss (e.g. LinearRegression() or Ridge() or even</span>
<span class="sd">        MLPRegressor ()). While any estimator providing the necessary prediction</span>
<span class="sd">        functions can be used, the convergence of the fixed point is not guaranteed</span>
<span class="sd">        and behavior can be unpredictable.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    base_estimator : object</span>
<span class="sd">        The base estimator to be used for the regression task. This estimator</span>
<span class="sd">        should solve a least squares regression problem (regularized or not)</span>
<span class="sd">        to correspond to JDOT theoretical regression problem but other</span>
<span class="sd">        approaches can be used with the risk that the fixed point might not converge.</span>
<span class="sd">    Xs : array-like of shape (n_samples, n_features)</span>
<span class="sd">        Source domain samples.</span>
<span class="sd">    ys : array-like of shape (n_samples,)</span>
<span class="sd">        Source domain labels.</span>
<span class="sd">    Xt : array-like of shape (m_samples, n_features)</span>
<span class="sd">        Target domain samples.</span>
<span class="sd">    alpha : float, default=0.5</span>
<span class="sd">        The trade-off parameter between the feature and label loss in OT metric</span>
<span class="sd">    ws : array-like of shape (n_samples,)</span>
<span class="sd">        Source domain weights (will ne normalized to sum to 1).</span>
<span class="sd">    wt : array-like of shape (m_samples,)</span>
<span class="sd">        Target domain weights (will ne normalized to sum to 1).</span>
<span class="sd">    n_iter_max: int</span>
<span class="sd">        Max number of JDOT alternat optimization iterations.</span>
<span class="sd">    tol: float&gt;0</span>
<span class="sd">        Tolerance for loss variations (OT and mse) stopping iterations.</span>
<span class="sd">    verbose: bool</span>
<span class="sd">        Print loss along iterations if True.as_integer_ratio</span>
<span class="sd">    kwargs : dict</span>
<span class="sd">        Additional parameters to be passed to the base estimator.</span>


<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    estimator : object</span>
<span class="sd">        The fitted estimator.</span>
<span class="sd">    lst_loss_ot : list</span>
<span class="sd">        The list of OT losses at each iteration.</span>
<span class="sd">    lst_loss_tgt_labels : list</span>
<span class="sd">        The list of target labels losses at each iteration.</span>
<span class="sd">    sol : object</span>
<span class="sd">        The solution of the OT problem.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    [10] N. Courty, R. Flamary, A. Habrard, A. Rakotomamonjy, Joint Distribution</span>
<span class="sd">        Optimal Transportation for Domain Adaptation, Neural Information Processing</span>
<span class="sd">        Systems (NIPS), 2017.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">estimator</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">)</span>

    <span class="c1"># compute feature distance matrix</span>
    <span class="n">Mf</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">Xs</span><span class="p">,</span> <span class="n">Xt</span><span class="p">)</span>
    <span class="n">Mf</span> <span class="o">=</span> <span class="n">Mf</span> <span class="o">/</span> <span class="n">Mf</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">nt</span> <span class="o">=</span> <span class="n">Xt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">ws</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">),))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">ws</span> <span class="o">/</span> <span class="n">ws</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">wt</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">nt</span><span class="p">,))</span> <span class="o">/</span> <span class="n">nt</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">wt</span> <span class="o">/</span> <span class="n">wt</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;sample_weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">wt</span>  <span class="c1"># add it as sample_weight for fit</span>

    <span class="n">lst_loss_ot</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lst_loss_tgt_labels</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">Ml</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">ys</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nt</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter_max</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># update the cost matrix</span>
            <span class="n">M</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">Mf</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">Ml</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">M</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">Mf</span>

        <span class="c1"># sole OT problem</span>
        <span class="n">sol</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

        <span class="n">T</span> <span class="o">=</span> <span class="n">sol</span><span class="o">.</span><span class="n">plan</span>
        <span class="n">loss_ot</span> <span class="o">=</span> <span class="n">sol</span><span class="o">.</span><span class="n">value</span>

        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">loss_ot</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Ml</span> <span class="o">*</span> <span class="n">T</span><span class="p">)</span>

        <span class="n">lst_loss_ot</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_ot</span><span class="p">)</span>

        <span class="c1"># compute the transported labels</span>
        <span class="n">yth</span> <span class="o">=</span> <span class="n">ys</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="n">b</span>

        <span class="c1"># fit the estimator</span>
        <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xt</span><span class="p">,</span> <span class="n">yth</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xt</span><span class="p">)</span>

        <span class="n">Ml</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">ys</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># compute the loss</span>
        <span class="n">loss_tgt_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">yth</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">lst_loss_tgt_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_tgt_labels</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;iter=</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, loss_ot=</span><span class="si">{</span><span class="n">loss_ot</span><span class="si">}</span><span class="s2">, loss_tgt_labels=</span><span class="si">{</span><span class="n">loss_tgt_labels</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># break on tol OT loss</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">abs</span><span class="p">(</span><span class="n">lst_loss_ot</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">lst_loss_ot</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="c1"># break on tol target loss</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">abs</span><span class="p">(</span><span class="n">lst_loss_tgt_labels</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">lst_loss_tgt_labels</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="c1"># update the cost matrix</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">n_iter_max</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Maximum number of iterations reached.&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">lst_loss_ot</span><span class="p">,</span> <span class="n">lst_loss_tgt_labels</span><span class="p">,</span> <span class="n">sol</span>


<span class="k">def</span> <span class="nf">solve_jdot_classification</span><span class="p">(</span>
    <span class="n">base_estimator</span><span class="p">,</span>
    <span class="n">Xs</span><span class="p">,</span>
    <span class="n">ys</span><span class="p">,</span>
    <span class="n">Xt</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">ws</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">wt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;multinomial&quot;</span><span class="p">,</span>
    <span class="n">n_iter_max</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">tol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">thr_weights</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Solve the joint distribution optimal transport classification problem [10]</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This estimator assumes that the loss function optimized by the base</span>
<span class="sd">        estimator is compatible with the given metric. For instance, if the</span>
<span class="sd">        metric is &#39;multinomial&#39;, the base estimator should optimize a</span>
<span class="sd">        cross-entropy loss (e.g. LogisticRegression with multi_class=&#39;multinomial&#39;)</span>
<span class="sd">        or a hinge loss (e.g. SVC with kernel=&#39;linear&#39; and one versus rest) if the</span>
<span class="sd">        metric is &#39;hinge&#39;. While any estimator providing the necessary prediction</span>
<span class="sd">        functions can be used, the convergence of the fixed point is not guaranteed</span>
<span class="sd">        and behavior can be unpredictable.</span>


<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    base_estimator : object</span>
<span class="sd">        The base estimator to be used for the classification task. This estimator</span>
<span class="sd">        should solve a classification problem to correspond to JDOT theoretical</span>
<span class="sd">        classification problem but other approaches can be used with the risk</span>
<span class="sd">        that the fixed point might not converge.</span>
<span class="sd">    Xs : array-like of shape (n_samples, n_features)</span>
<span class="sd">        Source domain samples.</span>
<span class="sd">    ys : array-like of shape (n_samples,)</span>
<span class="sd">        Source domain labels.</span>
<span class="sd">    Xt : array-like of shape (m_samples, n_features)</span>
<span class="sd">        Target domain samples.</span>
<span class="sd">    alpha : float, default=0.5</span>
<span class="sd">        The trade-off parameter between the feature and label loss in OT metric</span>
<span class="sd">    ws : array-like of shape (n_samples,)</span>
<span class="sd">        Source domain weights (will ne normalized to sum to 1).</span>
<span class="sd">    wt : array-like of shape (m_samples,)</span>
<span class="sd">        Target domain weights (will ne normalized to sum to 1).</span>
<span class="sd">    metric : str, default=&#39;multinomial&#39;</span>
<span class="sd">        The metric to use for the cost matrix. Can be &#39;multinomial&#39; for</span>
<span class="sd">        cross-entropy cost/ multinomial logistic regression or &#39;hinge&#39; for</span>
<span class="sd">        hinge cost (SVM/SVC).</span>
<span class="sd">    n_iter_max: int</span>
<span class="sd">        Max number of JDOT alternate optimization iterations.</span>
<span class="sd">    tol: float&gt;0</span>
<span class="sd">        Tolerance for loss variations (OT and mse) stopping iterations.</span>
<span class="sd">    verbose: bool</span>
<span class="sd">        Print loss along iterations if True.as_integer_ratio</span>
<span class="sd">    thr_weights : float, default=1e-6</span>
<span class="sd">        The relative threshold for the weights</span>
<span class="sd">    kwargs : dict</span>
<span class="sd">        Additional parameters to be passed to the base estimator.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    estimator : object</span>
<span class="sd">        The fitted estimator.</span>
<span class="sd">    lst_loss_ot : list</span>
<span class="sd">        The list of OT losses at each iteration.</span>
<span class="sd">    lst_loss_tgt_labels : list</span>
<span class="sd">        The list of target labels losses at each iteration.</span>
<span class="sd">    sol : object</span>
<span class="sd">        The solution of the OT problem.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    [10] N. Courty, R. Flamary, A. Habrard, A. Rakotomamonjy, Joint Distribution</span>
<span class="sd">         Optimal Transportation for Domain Adaptation, Neural Information Processing</span>
<span class="sd">         Systems (NIPS), 2017.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">estimator</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">)</span>

    <span class="c1"># compute feature distance matrix</span>
    <span class="n">Mf</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">Xs</span><span class="p">,</span> <span class="n">Xt</span><span class="p">)</span>
    <span class="n">Mf</span> <span class="o">=</span> <span class="n">Mf</span> <span class="o">/</span> <span class="n">Mf</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">nt</span> <span class="o">=</span> <span class="n">Xt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">ws</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">),))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">ws</span> <span class="o">/</span> <span class="n">ws</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">wt</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">nt</span><span class="p">,))</span> <span class="o">/</span> <span class="n">nt</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">wt</span> <span class="o">/</span> <span class="n">wt</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="n">encoder</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">sparse_output</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">Ys</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">ys</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">categories_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">lst_loss_ot</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lst_loss_tgt_labels</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">Ml</span> <span class="o">=</span> <span class="n">get_jdot_class_cost_matrix</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">Xt</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter_max</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># update the cost matrix</span>
            <span class="n">M</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">Mf</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">Ml</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">M</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">Mf</span>

        <span class="c1"># sole OT problem</span>
        <span class="n">sol</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

        <span class="n">T</span> <span class="o">=</span> <span class="n">sol</span><span class="o">.</span><span class="n">plan</span>
        <span class="n">loss_ot</span> <span class="o">=</span> <span class="n">sol</span><span class="o">.</span><span class="n">value</span>

        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">loss_ot</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Ml</span> <span class="o">*</span> <span class="n">T</span><span class="p">)</span>

        <span class="n">lst_loss_ot</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_ot</span><span class="p">)</span>

        <span class="c1"># compute the transported labels</span>
        <span class="n">Yth</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Ys</span><span class="p">)</span> <span class="o">*</span> <span class="n">nt</span>  <span class="c1"># not normalized because weights used in fit</span>

        <span class="c1"># create reweighted taregt data for classification</span>
        <span class="n">Xh</span><span class="p">,</span> <span class="n">yh</span><span class="p">,</span> <span class="n">wh</span> <span class="o">=</span> <span class="n">get_data_jdot_class</span><span class="p">(</span><span class="n">Xt</span><span class="p">,</span> <span class="n">Yth</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">thr_weights</span><span class="o">=</span><span class="n">thr_weights</span><span class="p">)</span>

        <span class="c1"># fit the estimator</span>
        <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xh</span><span class="p">,</span> <span class="n">yh</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">wh</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">Ml</span> <span class="o">=</span> <span class="n">get_jdot_class_cost_matrix</span><span class="p">(</span><span class="n">Ys</span><span class="p">,</span> <span class="n">Xt</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">)</span>

        <span class="c1"># compute the losses</span>
        <span class="n">loss_tgt_labels</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">get_tgt_loss_jdot_class</span><span class="p">(</span>
                <span class="n">Xh</span><span class="p">,</span> <span class="n">encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">yh</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]),</span> <span class="n">wh</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span>
            <span class="p">)</span>
            <span class="o">/</span> <span class="n">nt</span>
        <span class="p">)</span>
        <span class="n">lst_loss_tgt_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_tgt_labels</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;iter=</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, loss_ot=</span><span class="si">{</span><span class="n">loss_ot</span><span class="si">}</span><span class="s2">, loss_tgt_labels=</span><span class="si">{</span><span class="n">loss_tgt_labels</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># break on tol OT loss</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">abs</span><span class="p">(</span><span class="n">lst_loss_ot</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">lst_loss_ot</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="c1"># break on tol target loss</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">abs</span><span class="p">(</span><span class="n">lst_loss_tgt_labels</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">lst_loss_tgt_labels</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="c1"># update the cost matrix</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">n_iter_max</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Maximum number of iterations reached.&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">lst_loss_ot</span><span class="p">,</span> <span class="n">lst_loss_tgt_labels</span><span class="p">,</span> <span class="n">sol</span>


<div class="viewcode-block" id="JDOTRegressor">
<a class="viewcode-back" href="../../gen_modules/skada.JDOTRegressor.html#skada.JDOTRegressor">[docs]</a>
<span class="k">class</span> <span class="nc">JDOTRegressor</span><span class="p">(</span><span class="n">DAEstimator</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Joint Distribution Optimal Transport Regressor proposed in [10]</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This estimator assumes that the loss function optimized by the base</span>
<span class="sd">        estimator is the quadratic loss. For instance, the base estimator should</span>
<span class="sd">        optimize and L2 loss (e.g. LinearRegression() or Ridge() or even</span>
<span class="sd">        MLPRegressor ()). While any estimator providing the necessary prediction</span>
<span class="sd">        functions can be used, the convergence of the fixed point is not guaranteed</span>
<span class="sd">        and behavior can be unpredictable.</span>


<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    base_estimator : object</span>
<span class="sd">        The base estimator to be used for the regression task. This estimator</span>
<span class="sd">        should solve a least squares regression problem (regularized or not)</span>
<span class="sd">        to correspond to JDOT theoretical regression problem but other</span>
<span class="sd">        approaches can be used with the risk that the fixed point might not</span>
<span class="sd">        converge. default value is LinearRegression() from scikit-learn.</span>
<span class="sd">    alpha : float, default=0.5</span>
<span class="sd">        The trade-off parameter between the feature and label loss in OT metric</span>
<span class="sd">    n_iter_max: int</span>
<span class="sd">        Max number of JDOT alternat optimization iterations.</span>
<span class="sd">    tol: float&gt;0</span>
<span class="sd">        Tolerance for loss variations (OT and mse) stopping iterations.</span>
<span class="sd">    verbose: bool</span>
<span class="sd">        Print loss along iterations if True.as_integer_ratio</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator_ : object</span>
<span class="sd">        The fitted estimator.</span>
<span class="sd">    lst_loss_ot_ : list</span>
<span class="sd">        The list of OT losses at each iteration.</span>
<span class="sd">    lst_loss_tgt_labels_ : list</span>
<span class="sd">        The list of target labels losses at each iteration.</span>
<span class="sd">    sol_ : object</span>
<span class="sd">        The solution of the OT problem.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    [10] N. Courty, R. Flamary, A. Habrard, A. Rakotomamonjy, Joint Distribution</span>
<span class="sd">         Optimal Transportation for Domain Adaptation, Neural Information</span>
<span class="sd">         Processing Systems (NIPS), 2017.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">base_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">n_iter_max</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">tol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">base_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">,</span> <span class="s2">&quot;fit&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span>
                <span class="n">base_estimator</span><span class="p">,</span> <span class="s2">&quot;predict&quot;</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;base_estimator must be a regressor with&quot;</span> <span class="s2">&quot; fit and predict methods&quot;</span>
                <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span> <span class="o">=</span> <span class="n">base_estimator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_max</span> <span class="o">=</span> <span class="n">n_iter_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_domain</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fit adaptation parameters&quot;&quot;&quot;</span>
        <span class="n">Xs</span><span class="p">,</span> <span class="n">Xt</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="n">ws</span><span class="p">,</span> <span class="n">wt</span> <span class="o">=</span> <span class="n">source_target_split</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">sample_domain</span><span class="o">=</span><span class="n">sample_domain</span>
        <span class="p">)</span>

        <span class="n">res</span> <span class="o">=</span> <span class="n">solve_jdot_regression</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span><span class="p">,</span>
            <span class="n">Xs</span><span class="p">,</span>
            <span class="n">ys</span><span class="p">,</span>
            <span class="n">Xt</span><span class="p">,</span>
            <span class="n">ws</span><span class="o">=</span><span class="n">ws</span><span class="p">,</span>
            <span class="n">wt</span><span class="o">=</span><span class="n">wt</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">n_iter_max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter_max</span><span class="p">,</span>
            <span class="n">tol</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
            <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">estimator_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lst_loss_ot_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lst_loss_tgt_labels_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sol_</span> <span class="o">=</span> <span class="n">res</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">sample_domain</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Predict using the model&quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sample_domain</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">sample_domain</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Source domain detected. Predictor is trained on target&quot;</span>
                <span class="s2">&quot;and prediction might be biased.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_domain</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the coefficient of determination R^2 of the prediction&quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sample_domain</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">sample_domain</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Source domain detected. Predictor is trained on target&quot;</span>
                <span class="s2">&quot;and score might be biased.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator_</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span></div>



<span class="k">class</span> <span class="nc">JDOTClassifier</span><span class="p">(</span><span class="n">DAEstimator</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Joint Distribution Optimal Transport Classifier proposed in [10]</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This estimator assumes that the loss function optimized by the base</span>
<span class="sd">        estimator is compatible with the given metric. For instance, if the</span>
<span class="sd">        metric is &#39;multinomial&#39;, the base estimator should optimize a</span>
<span class="sd">        cross-entropy loss (e.g. LogisticRegression with multi_class=&#39;multinomial&#39;)</span>
<span class="sd">        or a hinge loss (e.g. SVC with kernel=&#39;linear&#39; and one versus rest) if the</span>
<span class="sd">        metric is &#39;hinge&#39;. While any estimator providing the necessary prediction</span>
<span class="sd">        functions can be used, the convergence of the fixed point is not guaranteed</span>
<span class="sd">        and behavior can be unpredictable.</span>


<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    base_estimator : object</span>
<span class="sd">        The base estimator to be used for the classification task. This</span>
<span class="sd">        estimator should solve a classification problem to correspond to JDOT</span>
<span class="sd">        theoretical classification problem but other approaches can be used with</span>
<span class="sd">        the risk that the fixed point might not converge. default value is</span>
<span class="sd">        LogisticRegression() from scikit-learn.</span>
<span class="sd">    alpha : float, default=0.5</span>
<span class="sd">        The trade-off parameter between the feature and label loss in OT metric</span>
<span class="sd">    metric : str, default=&#39;multinomial&#39;</span>
<span class="sd">        The metric to use for the cost matrix. Can be &#39;multinomial&#39; for</span>
<span class="sd">        cross-entropy cost/ multinomial logistic regression or &#39;hinge&#39; for hinge</span>
<span class="sd">        cost (SVM/SVC).</span>
<span class="sd">    n_iter_max: int</span>
<span class="sd">        Max number of JDOT alternat optimization iterations.</span>
<span class="sd">    tol: float&gt;0</span>
<span class="sd">        Tolerance for loss variations (OT and mse) stopping iterations.</span>
<span class="sd">    verbose: bool</span>
<span class="sd">        Print loss along iterations if True.as_integer_ratio</span>
<span class="sd">    thr_weights : float, default=1e-6</span>
<span class="sd">        The relative threshold for the weights</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator_ : object</span>
<span class="sd">        The fitted estimator.</span>
<span class="sd">    lst_loss_ot_ : list</span>
<span class="sd">        The list of OT losses at each iteration.</span>
<span class="sd">    lst_loss_tgt_labels_ : list</span>
<span class="sd">        The list of target labels losses at each iteration.</span>
<span class="sd">    sol_ : object</span>
<span class="sd">        The solution of the OT problem.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    [10] N. Courty, R. Flamary, A. Habrard, A. Rakotomamonjy, Joint Distribution</span>
<span class="sd">         Optimal Transportation for Domain Adaptation, Neural Information</span>
<span class="sd">         Processing Systems (NIPS), 2017.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">base_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;multinomial&quot;</span><span class="p">,</span>
        <span class="n">n_iter_max</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">tol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">thr_weights</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">base_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">multi_class</span><span class="o">=</span><span class="s2">&quot;multinomial&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">,</span> <span class="s2">&quot;fit&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span>
                <span class="n">base_estimator</span><span class="p">,</span> <span class="s2">&quot;predict&quot;</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;base_estimator must be a regressor with&quot;</span> <span class="s2">&quot; fit and predict methods&quot;</span>
                <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span> <span class="o">=</span> <span class="n">base_estimator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metric</span> <span class="o">=</span> <span class="n">metric</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_max</span> <span class="o">=</span> <span class="n">n_iter_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thr_weights</span> <span class="o">=</span> <span class="n">thr_weights</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_domain</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fit adaptation parameters&quot;&quot;&quot;</span>
        <span class="n">Xs</span><span class="p">,</span> <span class="n">Xt</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="n">ws</span><span class="p">,</span> <span class="n">wt</span> <span class="o">=</span> <span class="n">source_target_split</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">sample_domain</span><span class="o">=</span><span class="n">sample_domain</span>
        <span class="p">)</span>

        <span class="n">res</span> <span class="o">=</span> <span class="n">solve_jdot_classification</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span><span class="p">,</span>
            <span class="n">Xs</span><span class="p">,</span>
            <span class="n">ys</span><span class="p">,</span>
            <span class="n">Xt</span><span class="p">,</span>
            <span class="n">ws</span><span class="o">=</span><span class="n">ws</span><span class="p">,</span>
            <span class="n">wt</span><span class="o">=</span><span class="n">wt</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">metric</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metric</span><span class="p">,</span>
            <span class="n">n_iter_max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter_max</span><span class="p">,</span>
            <span class="n">tol</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">thr_weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">thr_weights</span><span class="p">,</span>
            <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">estimator_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lst_loss_ot_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lst_loss_tgt_labels_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sol_</span> <span class="o">=</span> <span class="n">res</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">sample_domain</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">allow_source</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Predict using the model&quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sample_domain</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">sample_domain</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Source domain detected. Predictor is trained on target&quot;</span>
                <span class="s2">&quot;and prediction might be biased.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span><span class="p">,</span> <span class="s2">&quot;predict_proba&quot;</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="s2">&quot;The base estimator does not have a predict_proba method&quot;</span>
            <span class="p">)</span>

    <span class="nd">@available_if</span><span class="p">(</span><span class="n">_check_proba</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">sample_domain</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">allow_source</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Predict using the model&quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sample_domain</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">sample_domain</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Source domain detected. Predictor is trained on target&quot;</span>
                <span class="s2">&quot;and prediction might be biased.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator_</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_domain</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the scores of the prediction&quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sample_domain</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">sample_domain</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Source domain detected. Predictor is trained on target&quot;</span>
                <span class="s2">&quot;and score might be biased.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator_</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">OTLabelPropAdapter</span><span class="p">(</span><span class="n">BaseAdapter</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Label propagation using optimal transport plan.</span>

<span class="sd">    This adapter uses the optimal transport plan to propagate labels from</span>
<span class="sd">    source to target domain. This was proposed originally in [28] for</span>
<span class="sd">    semi-supervised learning and can be used for domain adaptation.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    metric : str, default=&#39;sqeuclidean&#39;</span>
<span class="sd">        The metric to use for the cost matrix. Can be &#39;sqeuclidean&#39; for</span>
<span class="sd">        squared euclidean distance, &#39;euclidean&#39; for euclidean distance,</span>
<span class="sd">    reg : float, default=None</span>
<span class="sd">        The entropic  regularization parameter for the optimal transport</span>
<span class="sd">        problem. If None, the exact OT is solved, else it is used to weight</span>
<span class="sd">        the entropy regularizationof the coupling matrix.</span>
<span class="sd">    n_iter_max: int</span>
<span class="sd">        Maximum number of iterations for the OT solver.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    G_ : array-like of shape (n_samples, m_samples)</span>
<span class="sd">        The optimal transport plan.</span>
<span class="sd">    Xt_ : array-like of shape (m_samples, n_features)</span>
<span class="sd">        The target domain samples.</span>
<span class="sd">    yht_ : array-like of shape (m_samples,)</span>
<span class="sd">        The transported source domain labels.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    [28] Solomon, J., Rustamov, R., Guibas, L., &amp; Butscher, A. (2014, January).</span>
<span class="sd">     Wasserstein propagation for semi-supervised learning. In International</span>
<span class="sd">     Conference on Machine Learning (pp. 306-314). PMLR.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__metadata_request__fit</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;sample_weight&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>
    <span class="n">__metadata_request__fit_transform</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;sample_weight&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;sqeuclidean&quot;</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_iter_max</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metric</span> <span class="o">=</span> <span class="n">metric</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">=</span> <span class="n">reg</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_max</span> <span class="o">=</span> <span class="n">n_iter_max</span>

    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_domain</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fit adaptation parameters&quot;&quot;&quot;</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_domain</span> <span class="o">=</span> <span class="n">check_X_y_domain</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_domain</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">Xs</span><span class="p">,</span> <span class="n">Xt</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="n">ws</span><span class="p">,</span> <span class="n">wt</span> <span class="o">=</span> <span class="n">source_target_split</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">sample_domain</span><span class="o">=</span><span class="n">sample_domain</span>
            <span class="p">)</span>
            <span class="n">ws</span> <span class="o">=</span> <span class="n">ws</span> <span class="o">/</span> <span class="n">ws</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="n">wt</span> <span class="o">=</span> <span class="n">wt</span> <span class="o">/</span> <span class="n">wt</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">Xs</span><span class="p">,</span> <span class="n">Xt</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">yt</span> <span class="o">=</span> <span class="n">source_target_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_domain</span><span class="o">=</span><span class="n">sample_domain</span><span class="p">)</span>
            <span class="n">ws</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">unif</span><span class="p">(</span><span class="n">Xs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">wt</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">unif</span><span class="p">(</span><span class="n">Xt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">M</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">Xs</span><span class="p">,</span> <span class="n">Xt</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metric</span><span class="p">)</span>
        <span class="n">G</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">ws</span><span class="p">,</span> <span class="n">wt</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reg</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter_max</span><span class="p">)</span><span class="o">.</span><span class="n">plan</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">discrete_</span> <span class="o">=</span> <span class="n">discrete</span> <span class="o">=</span> <span class="n">_find_y_type</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span> <span class="o">==</span> <span class="n">Y_Type</span><span class="o">.</span><span class="n">DISCRETE</span>
        <span class="k">if</span> <span class="n">discrete</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
            <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Xs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">)))</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">classes</span><span class="p">):</span>
                <span class="n">Y</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">ys</span> <span class="o">==</span> <span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="n">yht</span> <span class="o">=</span> <span class="n">G</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">yht_continuous_</span> <span class="o">=</span> <span class="n">yht</span>
            <span class="n">yht</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">yht</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">yht</span> <span class="o">=</span> <span class="n">classes</span><span class="p">[</span><span class="n">yht</span><span class="p">]</span>
            <span class="n">yout</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">Y</span> <span class="o">=</span> <span class="n">ys</span>
            <span class="n">yht</span> <span class="o">=</span> <span class="n">G</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">/</span> <span class="n">wt</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">yht_continuous_</span> <span class="o">=</span> <span class="n">yht</span>
            <span class="n">yout</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">G_</span> <span class="o">=</span> <span class="n">G</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Xt_</span> <span class="o">=</span> <span class="n">Xt</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">yht_</span> <span class="o">=</span> <span class="n">yht</span>

        <span class="c1"># set estimated labels</span>
        <span class="n">yout</span><span class="p">[</span><span class="n">sample_domain</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">yht</span>

        <span class="c1"># return sample weight only if it was provided</span>
        <span class="n">dico</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dico</span><span class="p">[</span><span class="s2">&quot;sample_weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sample_weight</span>

        <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">yout</span><span class="p">,</span> <span class="n">dico</span>


<span class="k">def</span> <span class="nf">OTLabelProp</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;sqeuclidean&quot;</span><span class="p">,</span> <span class="n">n_iter_max</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Label propagation using optimal transport plan.</span>

<span class="sd">    This adapter uses the optimal transport plan to propagate labels from</span>
<span class="sd">    source to target domain. This was proposed originally in [28] for</span>
<span class="sd">    semi-supervised learning and can be used for domain adaptation.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    base_estimator : object</span>
<span class="sd">        The base estimator to be used for the classification task. This</span>
<span class="sd">        estimator should optimize a classification loss corresponding to the</span>
<span class="sd">        given metric and provide compatible predict method (decision_function of</span>
<span class="sd">        predict_proba).</span>
<span class="sd">    reg : float, default=0</span>
<span class="sd">        The entropic  regularization parameter for the optimal transport</span>
<span class="sd">        problem. If None, the exact OT is solved, else it is used to weight</span>
<span class="sd">        the entropy regularizationof the coupling matrix.</span>
<span class="sd">    metric : str, default=&#39;sqeuclidean&#39;</span>
<span class="sd">        The metric to use for the cost matrix. Can be &#39;sqeuclidean&#39; for</span>
<span class="sd">        squared euclidean distance, &#39;euclidean&#39; for euclidean distance,</span>
<span class="sd">    n_iter_max: int</span>
<span class="sd">        Maximum number of iterations for the OT solver.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    adapter : OTLabelPropAdapter</span>
<span class="sd">        The optimal transport label propagation adapter.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    [28] Solomon, J., Rustamov, R., Guibas, L., &amp; Butscher, A. (2014, January).</span>
<span class="sd">     Wasserstein propagation for semi-supervised learning. In International</span>
<span class="sd">     Conference on Machine Learning (pp. 306-314). PMLR.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">base_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;rbf&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">set_fit_request</span><span class="p">(</span><span class="n">sample_weight</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">make_da_pipeline</span><span class="p">(</span>
        <span class="n">OTLabelPropAdapter</span><span class="p">(</span><span class="n">reg</span><span class="o">=</span><span class="n">reg</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span> <span class="n">n_iter_max</span><span class="o">=</span><span class="n">n_iter_max</span><span class="p">),</span>
        <span class="n">base_estimator</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">class</span> <span class="nc">JCPOTLabelPropAdapter</span><span class="p">(</span><span class="n">BaseAdapter</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;JCPOT Label Propagation Adapter for multi source target shift</span>

<span class="sd">    This adapter uses the optimal transport plan to propagate labels from</span>
<span class="sd">    sources to target domain with target shift (change in proportion of</span>
<span class="sd">    classes). This was proposed in [31].</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    metric : str, default=&#39;sqeuclidean&#39;</span>
<span class="sd">        The metric to use for the cost matrix. Can be &#39;sqeuclidean&#39; for</span>
<span class="sd">        squared euclidean distance, &#39;euclidean&#39; for euclidean distance,</span>
<span class="sd">    reg : float, default=1</span>
<span class="sd">        The entropic  regularization parameter for the optimal transport</span>
<span class="sd">        problem.</span>
<span class="sd">    max_iter : int, default=10</span>
<span class="sd">        Maximum number of iterations for the JCPOT solver.</span>
<span class="sd">    tol : float, default=1e-9</span>
<span class="sd">        Tolerance for loss variations (OT and mse) stopping iterations.</span>
<span class="sd">    verbose : bool, default=False</span>
<span class="sd">        Print loss along iterations if True.</span>


<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    [31] Redko, Ievgen, Nicolas Courty, Rémi Flamary, and Devis Tuia. &quot;Optimal</span>
<span class="sd">         transport for multi-source domain adaptation under target shift.&quot; In</span>
<span class="sd">         The 22nd International Conference on artificial intelligence and</span>
<span class="sd">         statistics, pp. 849-858. PMLR, 2019.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;sqeuclidean&quot;</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metric</span> <span class="o">=</span> <span class="n">metric</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">=</span> <span class="n">reg</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_domain</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_domain</span> <span class="o">=</span> <span class="n">check_X_y_domain</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_domain</span><span class="p">)</span>

        <span class="n">sources</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">per_domain_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_domain</span><span class="o">=</span><span class="n">sample_domain</span><span class="p">)</span>

        <span class="n">Xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span> <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">sources</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
        <span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="n">y</span> <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">sources</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">Xs</span> <span class="o">=</span> <span class="n">Xs</span> <span class="o">*</span> <span class="mi">2</span>
            <span class="n">ys</span> <span class="o">=</span> <span class="n">ys</span> <span class="o">*</span> <span class="mi">2</span>

        <span class="n">Xt</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span> <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">targets</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>

        <span class="n">Xt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">Xt</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ot_adapter_</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">da</span><span class="o">.</span><span class="n">JCPOTTransport</span><span class="p">(</span>
            <span class="n">reg_e</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reg</span><span class="p">,</span>
            <span class="n">metric</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metric</span><span class="p">,</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">tol</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
            <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ot_adapter_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xs</span><span class="o">=</span><span class="n">Xs</span><span class="p">,</span> <span class="n">ys</span><span class="o">=</span><span class="n">ys</span><span class="p">,</span> <span class="n">Xt</span><span class="o">=</span><span class="n">Xt</span><span class="p">)</span>

        <span class="n">yh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ot_adapter_</span><span class="o">.</span><span class="n">transform_labels</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">yh_continuous_</span> <span class="o">=</span> <span class="n">yh</span>

        <span class="n">yh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">yh</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">yout</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">yout</span><span class="p">[</span><span class="n">sample_domain</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">yh</span>

        <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">yout</span><span class="p">,</span> <span class="p">{}</span>


<span class="k">def</span> <span class="nf">JCPOTLabelProp</span><span class="p">(</span>
    <span class="n">base_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">reg</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;sqeuclidean&quot;</span><span class="p">,</span>
    <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">tol</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;JCPOT Label Propagation Adapter for multi source target shift</span>

<span class="sd">    This adapter uses the optimal transport plan to propagate labels from</span>
<span class="sd">    sources to target domain with target shift (change in proportion of</span>
<span class="sd">    classes). This was proposed in [31].</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    base_estimator : object, default=LinearRegression()</span>
<span class="sd">        The base estimator to be used for the classification task. This</span>
<span class="sd">        estimator should optimize a classification loss corresponding to the</span>
<span class="sd">        given metric and provide compatible predict method (decision_function of</span>
<span class="sd">        predict_proba).</span>
<span class="sd">    reg : float, default=1</span>
<span class="sd">        The entropic  regularization parameter for the optimal transport</span>
<span class="sd">        problem.</span>
<span class="sd">    metric : str, default=&#39;sqeuclidean&#39;</span>
<span class="sd">        The metric to use for the cost matrix. Can be &#39;sqeuclidean&#39; for</span>
<span class="sd">        squared euclidean distance, &#39;euclidean&#39; for euclidean distance,</span>
<span class="sd">    max_iter : int, default=10</span>
<span class="sd">        Maximum number of iterations for the JCPOT solver.</span>
<span class="sd">    tol : float, default=1e-9</span>
<span class="sd">        Tolerance for loss variations (OT and mse) stopping iterations.</span>
<span class="sd">    verbose : bool, default=False</span>
<span class="sd">        Print loss along iterations if True.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    adapter : JCPOTLabelPropAdapter</span>
<span class="sd">        The optimal transport label propagation adapter.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    [31] Redko, Ievgen, Nicolas Courty, Rémi Flamary, and Devis Tuia. &quot;Optimal</span>
<span class="sd">         transport for multi-source domain adaptation under target shift.&quot; In</span>
<span class="sd">         The 22nd International Conference on artificial intelligence and</span>
<span class="sd">         statistics, pp. 849-858. PMLR, 2019.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">base_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">make_da_pipeline</span><span class="p">(</span>
        <span class="n">JCPOTLabelPropAdapter</span><span class="p">(</span>
            <span class="n">reg</span><span class="o">=</span><span class="n">reg</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span>
        <span class="p">),</span>
        <span class="n">base_estimator</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, The SKADA team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <div class="rst-versions" data-toggle="rst-versions" role="note"
aria-label="versions">
  <!--  add shift_up to the class for force viewing ,
  data-toggle="rst-current-version" -->
    <span class="rst-current-version"  style="margin-bottom:1mm;">
      <span class="fa fa-book"> SKADA: SciKit-ADAptation</span> 0.4.0
      <hr  style="margin-bottom:1.5mm;margin-top:5mm;">
     <!--  versions
      <span class="fa fa-caret-down"></span>-->
      <span class="rst-current-version" style="display: inline-block;padding:
      0px;color:#fcfcfcab;float:left;font-size: 100%;">
        Versions: 
        <a href="https://scikit-adaptation.github.io/" 
        style="padding: 3px;color:#fcfcfc;font-size: 100%;">Release</a>
        <a href="https://scikit-adaptation.github.io/dev" 
        style="padding: 3px;color:#fcfcfc;font-size: 100%;">Development</a>
        <a href="https://github.com/scikit-adaptation/skada"
        style="padding: 3px;color:#fcfcfc;font-size: 100%;">Code</a>

      </span>

     
    </span>
  
     <!--
    <div class="rst-other-versions">

      

<div class="injected">

    
  <dl>
    <dt>Versions</dt>

    <dd><a href="https://pythonot.github.io/">Release</a></dd>
    
    <dd><a href="https://pythonot.github.io/master">Development</a></dd>
   
    

    <dt><a href="https://github.com/PythonOT/POT">Code on Github</a></dt>       

    
  </dl>
  <hr>

</div> 
</div>-->
  </div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>